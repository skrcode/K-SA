{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import sys\n",
    "import glob\n",
    "import errno\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk.data\n",
    "import nltk\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from subprocess import check_call\n",
    "from shutil import copyfile\n",
    "from sklearn.metrics import log_loss\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalAveragePooling1D,Merge,Lambda,Input,GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,TimeDistributed\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from sklearn.linear_model import SGDClassifier as sgd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "train = \"../data/train.csv\"\n",
    "test = \"../data/test.csv\"\n",
    "wv = \"../../../../glove.6B/glove.6B.100d.txt\"\n",
    "X_train = pd.read_csv( train, header=0,delimiter=\",\" )\n",
    "X_test = pd.read_csv( test, header=0,delimiter=\",\" )\n",
    "\n",
    "word_vecs = {}\n",
    "with open(wv) as f:\n",
    "    for line in f:\n",
    "       vals = line.split()\n",
    "       word_vecs[vals[0]] = np.array(vals[1::],dtype=float)\n",
    "authors = ['EAP','MWS','HPL']\n",
    "\n",
    "Y_train = LabelEncoder().fit_transform(X_train['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean data\n",
    "def getWordVectors(X_train,X_test,word_vecs):\n",
    "    X_train['word_vectors'] = [ [ word_vecs[word] for word in sentence if word in word_vecs] for sentence in X_train['text']]\n",
    "    X_test['word_vectors'] = [ [ word_vecs[word] for word in sentence if word in word_vecs] for sentence in X_test['text']] \n",
    "    return X_train,X_test\n",
    "\n",
    "def getSentenceVectors(X_train,X_test):\n",
    "    X_train['sentence_vectors'] =[np.mean(sentence,axis = 0) for sentence in X_train['word_vectors']]\n",
    "    X_test['sentence_vectors'] =[np.mean(sentence,axis = 0) for sentence in X_test['word_vectors']] \n",
    "    return X_train,X_test\n",
    "\n",
    "def clean(X_train,X_test):\n",
    "    X_train['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_train['text']]\n",
    "    X_test['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_test['text']]\n",
    "    return X_train,X_test\n",
    "X_train,X_test = clean(X_train,X_test)\n",
    "X_train,X_test = getWordVectors(X_train,X_test,word_vecs)\n",
    "X_train,X_test = getSentenceVectors(X_train,X_test)\n",
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Punctuation\n",
    "punctuations = [{\"id\":1,\"p\":\"[;:]\"},{\"id\":2,\"p\":\"[,.]\"},{\"id\":3,\"p\":\"[?]\"},{\"id\":4,\"p\":\"[\\']\"},{\"id\":5,\"p\":\"[\\\"]\"},{\"id\":6,\"p\":\"[;:,.?\\'\\\"]\"}]\n",
    "for p in punctuations:\n",
    "    punctuation = p[\"p\"]\n",
    "    _train =  [ sentence.split() for sentence in X_train['text'] ]\n",
    "    X_train['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _train]    \n",
    "\n",
    "    _test =  [ sentence.split() for sentence in X_test['text'] ]\n",
    "    X_test['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _test]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Stop Words\n",
    "_dist_train = [x for x in X_train['words']]\n",
    "X_train['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/len(sentence) for sentence in _dist_train]\n",
    "\n",
    "_dist_test = [x for x in X_test['words']]\n",
    "X_test['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/len(sentence) for sentence in _dist_test]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean cv score : ', 0.84221619836128525)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# tfidf - words - nb+svd\n",
    "def tfidfWords(X_train,X_test):\n",
    "    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_tfidf,test_tfidf,full_tfidf\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def runSVD(full_tfidf,train_tfidf,test_tfidf):   \n",
    "    n_comp = 20\n",
    "    svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "    svd_obj.fit(full_tfidf)\n",
    "    train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "    test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "\n",
    "    train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "    test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "    return train_svd,test_svd\n",
    "\n",
    "def do_tfidf_MNB(X_train,X_test,Y_train):\n",
    "    train_tfidf,test_tfidf,full_tfidf = tfidfWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "def do_tfidf_SVD(X_train,X_test,Y_train):\n",
    "    train_tfidf,test_tfidf,full_tfidf = tfidfWords(X_train,X_test)\n",
    "    train_svd,test_svd = runSVD(full_tfidf,train_tfidf,test_tfidf)\n",
    "    return train_svd,test_svd\n",
    "\n",
    "pred_train,pred_test = do_tfidf_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"tfidf_words_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"tfidf_words_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"tfidf_words_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"tfidf_words_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"tfidf_words_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"tfidf_words_nb_mws\"] = pred_test[:,2]\n",
    "\n",
    "# pred_train,pred_test = do_tfidf_SVD(X_train,X_test,Y_train)\n",
    "# print pred_train\n",
    "# # X_train[\"tfidf_words_nb_eap\"] = pred_train[:,0]\n",
    "# # X_train[\"tfidf_words_nb_hpl\"] = pred_train[:,1]\n",
    "# # X_train[\"tfidf_words_nb_mws\"] = pred_train[:,2]\n",
    "# # X_test[\"tfidf_words_nb_eap\"] = pred_test[:,0]\n",
    "# # X_test[\"tfidf_words_nb_hpl\"] = pred_test[:,1]\n",
    "# # X_test[\"tfidf_words_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean cv score : ', 0.7904152589474216)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# tfidf - chars - nb+svd\n",
    "def tfidfWords(X_train,X_test):\n",
    "    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,5),analyzer='char')\n",
    "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_tfidf,test_tfidf\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(X_train,X_test,Y_train):\n",
    "    train_tfidf,test_tfidf = tfidfWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "pred_train,pred_test = do(X_train,X_test,Y_train)\n",
    "X_train[\"tfidf_chars_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"tfidf_chars_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"tfidf_chars_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"tfidf_chars_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"tfidf_chars_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"tfidf_chars_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean cv score : ', 0.45091841616567468)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# count - words - nb\n",
    "def countWords(X_train,X_test):\n",
    "    count_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_count = count_vec.transform(X_train['text'].values.tolist())\n",
    "    test_count = count_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_count,test_count\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_count_MNB(X_train,X_test,Y_train):\n",
    "    train_count,test_count=countWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "pred_train,pred_test = do_count_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"count_words_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"count_words_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"count_words_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"count_words_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"count_words_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"count_words_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean cv score : ', 3.750763922681903)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# count - chars - nb\n",
    "def countChars(X_train,X_test):\n",
    "    count_vec = CountVectorizer(ngram_range=(1,7),analyzer='char')\n",
    "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_count = count_vec.transform(X_train['text'].values.tolist())\n",
    "    test_count = count_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_count,test_count\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_count_chars_MNB(X_train,X_test,Y_train):\n",
    "    train_count,test_count=countChars(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "pred_train,pred_test = do_count_chars_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"count_chars_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"count_chars_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"count_chars_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"count_chars_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"count_chars_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"count_chars_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/19579 [00:00<05:55, 54.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19579/19579 [01:54<00:00, 170.79it/s]\n",
      "100%|██████████| 8392/8392 [00:48<00:00, 171.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "def loadWordVecs():\n",
    "    embeddings_index = {}\n",
    "    f = open(wv)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def sent2vec(embeddings_index,s): # this function creates a normalized vector for the whole sentence\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stopwords.words('english')]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "def doGlove(x_train,x_test):\n",
    "    embeddings_index = loadWordVecs()\n",
    "    # create sentence vectors using the above function for training and validation set\n",
    "    xtrain_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_train)]\n",
    "    xtest_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_test)]\n",
    "    xtrain_glove = np.array(xtrain_glove)\n",
    "    xtest_glove = np.array(xtest_glove)\n",
    "    return xtrain_glove,xtest_glove,embeddings_index\n",
    "\n",
    "glove_vecs_train,glove_vecs_test,embeddings_index = doGlove(X_train['text'],X_test['text'])\n",
    "X_train[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_train.tolist())\n",
    "X_test[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_test.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 7s 438us/step - loss: 1.0787 - acc: 0.4064 - val_loss: 1.0700 - val_acc: 0.3930\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 5s 312us/step - loss: 1.0273 - acc: 0.4444 - val_loss: 0.9949 - val_acc: 0.4814\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 5s 311us/step - loss: 0.9158 - acc: 0.6389 - val_loss: 0.8830 - val_acc: 0.6555\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 5s 316us/step - loss: 0.7845 - acc: 0.7697 - val_loss: 0.7764 - val_acc: 0.7518\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 5s 322us/step - loss: 0.6697 - acc: 0.8163 - val_loss: 0.6929 - val_acc: 0.7748\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 5s 329us/step - loss: 0.5767 - acc: 0.8521 - val_loss: 0.6279 - val_acc: 0.7919\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 5s 319us/step - loss: 0.5011 - acc: 0.8736 - val_loss: 0.5765 - val_acc: 0.8090\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 5s 327us/step - loss: 0.4380 - acc: 0.8914 - val_loss: 0.5350 - val_acc: 0.8287\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 5s 344us/step - loss: 0.3846 - acc: 0.9095 - val_loss: 0.5015 - val_acc: 0.8274\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 5s 328us/step - loss: 0.3388 - acc: 0.9214 - val_loss: 0.4716 - val_acc: 0.8414\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 5s 342us/step - loss: 0.2991 - acc: 0.9338 - val_loss: 0.4493 - val_acc: 0.8514\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 5s 343us/step - loss: 0.2640 - acc: 0.9435 - val_loss: 0.4268 - val_acc: 0.8524\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 5s 347us/step - loss: 0.2339 - acc: 0.9522 - val_loss: 0.4113 - val_acc: 0.8544\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 5s 350us/step - loss: 0.2066 - acc: 0.9604 - val_loss: 0.3948 - val_acc: 0.8590\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 5s 338us/step - loss: 0.1832 - acc: 0.9665 - val_loss: 0.3819 - val_acc: 0.8641\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 5s 336us/step - loss: 0.1620 - acc: 0.9717 - val_loss: 0.3725 - val_acc: 0.8654\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 5s 341us/step - loss: 0.1437 - acc: 0.9760 - val_loss: 0.3625 - val_acc: 0.8670\n",
      "Epoch 18/25\n",
      "15663/15663 [==============================] - 5s 341us/step - loss: 0.1273 - acc: 0.9802 - val_loss: 0.3576 - val_acc: 0.8647\n",
      "Epoch 19/25\n",
      "15663/15663 [==============================] - 5s 335us/step - loss: 0.1127 - acc: 0.9819 - val_loss: 0.3510 - val_acc: 0.8677\n",
      "Epoch 20/25\n",
      "15663/15663 [==============================] - 5s 339us/step - loss: 0.0995 - acc: 0.9846 - val_loss: 0.3477 - val_acc: 0.8700\n",
      "Epoch 21/25\n",
      "15663/15663 [==============================] - 5s 341us/step - loss: 0.0889 - acc: 0.9878 - val_loss: 0.3410 - val_acc: 0.8718\n",
      "Epoch 22/25\n",
      "15663/15663 [==============================] - 5s 330us/step - loss: 0.0783 - acc: 0.9892 - val_loss: 0.3401 - val_acc: 0.8695\n",
      "Epoch 23/25\n",
      "15663/15663 [==============================] - 5s 340us/step - loss: 0.0698 - acc: 0.9904 - val_loss: 0.3366 - val_acc: 0.8703\n",
      "Epoch 24/25\n",
      "15663/15663 [==============================] - 5s 335us/step - loss: 0.0621 - acc: 0.9920 - val_loss: 0.3344 - val_acc: 0.8705\n",
      "Epoch 25/25\n",
      "15663/15663 [==============================] - 5s 343us/step - loss: 0.0548 - acc: 0.9932 - val_loss: 0.3396 - val_acc: 0.8708\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 7s 449us/step - loss: 1.0797 - acc: 0.3990 - val_loss: 1.0604 - val_acc: 0.4114\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 5s 330us/step - loss: 1.0221 - acc: 0.4535 - val_loss: 0.9827 - val_acc: 0.5151\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 5s 309us/step - loss: 0.9049 - acc: 0.6492 - val_loss: 0.8699 - val_acc: 0.6782\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 5s 309us/step - loss: 0.7710 - acc: 0.7795 - val_loss: 0.7714 - val_acc: 0.7740\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 5s 299us/step - loss: 0.6545 - acc: 0.8300 - val_loss: 0.6880 - val_acc: 0.7855\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 5s 294us/step - loss: 0.5605 - acc: 0.8624 - val_loss: 0.6283 - val_acc: 0.8041\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 5s 299us/step - loss: 0.4846 - acc: 0.8818 - val_loss: 0.5806 - val_acc: 0.8166\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 5s 293us/step - loss: 0.4228 - acc: 0.9020 - val_loss: 0.5415 - val_acc: 0.8238\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 5s 287us/step - loss: 0.3701 - acc: 0.9152 - val_loss: 0.5075 - val_acc: 0.8233\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 5s 292us/step - loss: 0.3262 - acc: 0.9270 - val_loss: 0.4816 - val_acc: 0.8335\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 5s 319us/step - loss: 0.2872 - acc: 0.9397 - val_loss: 0.4586 - val_acc: 0.8414\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 5s 292us/step - loss: 0.2535 - acc: 0.9482 - val_loss: 0.4412 - val_acc: 0.8361\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 5s 290us/step - loss: 0.2243 - acc: 0.9549 - val_loss: 0.4238 - val_acc: 0.8440\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 5s 307us/step - loss: 0.1982 - acc: 0.9612 - val_loss: 0.4131 - val_acc: 0.8442\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 5s 320us/step - loss: 0.1756 - acc: 0.9679 - val_loss: 0.3977 - val_acc: 0.8542\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 5s 302us/step - loss: 0.1553 - acc: 0.9728 - val_loss: 0.3869 - val_acc: 0.8537\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 5s 298us/step - loss: 0.1378 - acc: 0.9776 - val_loss: 0.3790 - val_acc: 0.8552\n",
      "Epoch 18/25\n",
      "15663/15663 [==============================] - 5s 300us/step - loss: 0.1223 - acc: 0.9796 - val_loss: 0.3717 - val_acc: 0.8573\n",
      "Epoch 19/25\n",
      "15663/15663 [==============================] - 5s 309us/step - loss: 0.1083 - acc: 0.9831 - val_loss: 0.3705 - val_acc: 0.8519\n",
      "Epoch 20/25\n",
      "15663/15663 [==============================] - 5s 293us/step - loss: 0.0963 - acc: 0.9843 - val_loss: 0.3600 - val_acc: 0.8598\n",
      "Epoch 21/25\n",
      "15663/15663 [==============================] - 4s 281us/step - loss: 0.0853 - acc: 0.9868 - val_loss: 0.3563 - val_acc: 0.8580\n",
      "Epoch 22/25\n",
      "15663/15663 [==============================] - 5s 298us/step - loss: 0.0756 - acc: 0.9891 - val_loss: 0.3748 - val_acc: 0.8498\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 8s 520us/step - loss: 1.0804 - acc: 0.4033 - val_loss: 1.0681 - val_acc: 0.4055\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 7s 449us/step - loss: 1.0346 - acc: 0.4366 - val_loss: 1.0036 - val_acc: 0.4581\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 6s 403us/step - loss: 0.9258 - acc: 0.6277 - val_loss: 0.8975 - val_acc: 0.7354\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 6s 403us/step - loss: 0.7908 - acc: 0.7748 - val_loss: 0.7907 - val_acc: 0.7451\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 7s 420us/step - loss: 0.6711 - acc: 0.8193 - val_loss: 0.7078 - val_acc: 0.7702\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 7s 433us/step - loss: 0.5760 - acc: 0.8484 - val_loss: 0.6443 - val_acc: 0.7748\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 6s 403us/step - loss: 0.4986 - acc: 0.8730 - val_loss: 0.5960 - val_acc: 0.7998\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 6s 372us/step - loss: 0.4354 - acc: 0.8909 - val_loss: 0.5537 - val_acc: 0.8046\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 6s 373us/step - loss: 0.3826 - acc: 0.9072 - val_loss: 0.5209 - val_acc: 0.8146\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 5s 349us/step - loss: 0.3367 - acc: 0.9219 - val_loss: 0.4948 - val_acc: 0.8187\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 6s 353us/step - loss: 0.2973 - acc: 0.9338 - val_loss: 0.4681 - val_acc: 0.8289\n",
      "Epoch 12/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 5s 351us/step - loss: 0.2629 - acc: 0.9428 - val_loss: 0.4494 - val_acc: 0.8396\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 6s 357us/step - loss: 0.2325 - acc: 0.9514 - val_loss: 0.4329 - val_acc: 0.8407\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 6s 353us/step - loss: 0.2061 - acc: 0.9596 - val_loss: 0.4211 - val_acc: 0.8376\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 6s 354us/step - loss: 0.1824 - acc: 0.9649 - val_loss: 0.4048 - val_acc: 0.8455\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 6s 352us/step - loss: 0.1618 - acc: 0.9700 - val_loss: 0.3932 - val_acc: 0.8493\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 6s 352us/step - loss: 0.1442 - acc: 0.9741 - val_loss: 0.3879 - val_acc: 0.8483\n",
      "Epoch 18/25\n",
      "15663/15663 [==============================] - 6s 354us/step - loss: 0.1278 - acc: 0.9773 - val_loss: 0.3780 - val_acc: 0.8511\n",
      "Epoch 19/25\n",
      "15663/15663 [==============================] - 5s 350us/step - loss: 0.1135 - acc: 0.9809 - val_loss: 0.3713 - val_acc: 0.8534\n",
      "Epoch 20/25\n",
      "15663/15663 [==============================] - 6s 359us/step - loss: 0.1013 - acc: 0.9826 - val_loss: 0.3651 - val_acc: 0.8547\n",
      "Epoch 21/25\n",
      "15663/15663 [==============================] - 6s 359us/step - loss: 0.0896 - acc: 0.9851 - val_loss: 0.3611 - val_acc: 0.8544\n",
      "Epoch 22/25\n",
      "15663/15663 [==============================] - 6s 359us/step - loss: 0.0798 - acc: 0.9872 - val_loss: 0.3580 - val_acc: 0.8580\n",
      "Epoch 23/25\n",
      "15663/15663 [==============================] - 6s 357us/step - loss: 0.0713 - acc: 0.9890 - val_loss: 0.3605 - val_acc: 0.8550\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 8s 480us/step - loss: 1.0807 - acc: 0.4011 - val_loss: 1.0671 - val_acc: 0.4047\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 6s 356us/step - loss: 1.0312 - acc: 0.4353 - val_loss: 0.9940 - val_acc: 0.4982\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 5s 340us/step - loss: 0.9190 - acc: 0.6367 - val_loss: 0.8784 - val_acc: 0.6902\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 5s 343us/step - loss: 0.7825 - acc: 0.7682 - val_loss: 0.7708 - val_acc: 0.7380\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 5s 335us/step - loss: 0.6633 - acc: 0.8262 - val_loss: 0.6831 - val_acc: 0.7789\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 5s 344us/step - loss: 0.5680 - acc: 0.8533 - val_loss: 0.6182 - val_acc: 0.7993\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 5s 346us/step - loss: 0.4908 - acc: 0.8782 - val_loss: 0.5747 - val_acc: 0.8044\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 5s 336us/step - loss: 0.4276 - acc: 0.8963 - val_loss: 0.5269 - val_acc: 0.8233\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 5s 345us/step - loss: 0.3748 - acc: 0.9118 - val_loss: 0.4976 - val_acc: 0.8289\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 5s 347us/step - loss: 0.3298 - acc: 0.9249 - val_loss: 0.4652 - val_acc: 0.8389\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 5s 346us/step - loss: 0.2899 - acc: 0.9363 - val_loss: 0.4423 - val_acc: 0.8417\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 5s 335us/step - loss: 0.2557 - acc: 0.9468 - val_loss: 0.4265 - val_acc: 0.8407\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 5s 338us/step - loss: 0.2259 - acc: 0.9537 - val_loss: 0.4054 - val_acc: 0.8519\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 5s 338us/step - loss: 0.1990 - acc: 0.9608 - val_loss: 0.3964 - val_acc: 0.8501\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 5s 335us/step - loss: 0.1766 - acc: 0.9669 - val_loss: 0.3806 - val_acc: 0.8606\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 6s 354us/step - loss: 0.1561 - acc: 0.9715 - val_loss: 0.3686 - val_acc: 0.8647\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 5s 340us/step - loss: 0.1383 - acc: 0.9746 - val_loss: 0.3612 - val_acc: 0.8636\n",
      "Epoch 18/25\n",
      "15663/15663 [==============================] - 5s 344us/step - loss: 0.1222 - acc: 0.9785 - val_loss: 0.3533 - val_acc: 0.8654\n",
      "Epoch 19/25\n",
      "15663/15663 [==============================] - 5s 338us/step - loss: 0.1083 - acc: 0.9821 - val_loss: 0.3486 - val_acc: 0.8649\n",
      "Epoch 20/25\n",
      "15663/15663 [==============================] - 5s 351us/step - loss: 0.0959 - acc: 0.9842 - val_loss: 0.3419 - val_acc: 0.8687\n",
      "Epoch 21/25\n",
      "15663/15663 [==============================] - 6s 353us/step - loss: 0.0846 - acc: 0.9865 - val_loss: 0.3409 - val_acc: 0.8641\n",
      "Epoch 22/25\n",
      "15663/15663 [==============================] - 5s 336us/step - loss: 0.0753 - acc: 0.9887 - val_loss: 0.3356 - val_acc: 0.8690\n",
      "Epoch 23/25\n",
      "15663/15663 [==============================] - 5s 337us/step - loss: 0.0665 - acc: 0.9906 - val_loss: 0.3339 - val_acc: 0.8687\n",
      "Epoch 24/25\n",
      "15663/15663 [==============================] - 5s 340us/step - loss: 0.0591 - acc: 0.9918 - val_loss: 0.3317 - val_acc: 0.8718\n",
      "Epoch 25/25\n",
      "15663/15663 [==============================] - 5s 338us/step - loss: 0.0522 - acc: 0.9928 - val_loss: 0.3320 - val_acc: 0.8705\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/25\n",
      "15664/15664 [==============================] - 7s 475us/step - loss: 1.0815 - acc: 0.3994 - val_loss: 1.0666 - val_acc: 0.4110\n",
      "Epoch 2/25\n",
      "15664/15664 [==============================] - 5s 339us/step - loss: 1.0364 - acc: 0.4293 - val_loss: 1.0011 - val_acc: 0.4784\n",
      "Epoch 3/25\n",
      "15664/15664 [==============================] - 5s 337us/step - loss: 0.9311 - acc: 0.6241 - val_loss: 0.8949 - val_acc: 0.6685\n",
      "Epoch 4/25\n",
      "15664/15664 [==============================] - 5s 338us/step - loss: 0.7989 - acc: 0.7663 - val_loss: 0.7903 - val_acc: 0.7216\n",
      "Epoch 5/25\n",
      "15664/15664 [==============================] - 5s 341us/step - loss: 0.6802 - acc: 0.8162 - val_loss: 0.7075 - val_acc: 0.7760\n",
      "Epoch 6/25\n",
      "15664/15664 [==============================] - 6s 355us/step - loss: 0.5845 - acc: 0.8461 - val_loss: 0.6439 - val_acc: 0.7791\n",
      "Epoch 7/25\n",
      "15664/15664 [==============================] - 6s 374us/step - loss: 0.5077 - acc: 0.8698 - val_loss: 0.5920 - val_acc: 0.7957\n",
      "Epoch 8/25\n",
      "15664/15664 [==============================] - 5s 341us/step - loss: 0.4443 - acc: 0.8874 - val_loss: 0.5515 - val_acc: 0.8064\n",
      "Epoch 9/25\n",
      "15664/15664 [==============================] - 5s 340us/step - loss: 0.3911 - acc: 0.9050 - val_loss: 0.5166 - val_acc: 0.8217\n",
      "Epoch 10/25\n",
      "15664/15664 [==============================] - 5s 338us/step - loss: 0.3451 - acc: 0.9191 - val_loss: 0.4896 - val_acc: 0.8222\n",
      "Epoch 11/25\n",
      "15664/15664 [==============================] - 5s 339us/step - loss: 0.3055 - acc: 0.9311 - val_loss: 0.4678 - val_acc: 0.8222\n",
      "Epoch 12/25\n",
      "15664/15664 [==============================] - 5s 340us/step - loss: 0.2708 - acc: 0.9409 - val_loss: 0.4423 - val_acc: 0.8373\n",
      "Epoch 13/25\n",
      "15664/15664 [==============================] - 5s 341us/step - loss: 0.2404 - acc: 0.9500 - val_loss: 0.4234 - val_acc: 0.8432\n",
      "Epoch 14/25\n",
      "15664/15664 [==============================] - 5s 341us/step - loss: 0.2134 - acc: 0.9566 - val_loss: 0.4081 - val_acc: 0.8490\n",
      "Epoch 15/25\n",
      "15664/15664 [==============================] - 5s 338us/step - loss: 0.1898 - acc: 0.9632 - val_loss: 0.3984 - val_acc: 0.8483\n",
      "Epoch 16/25\n",
      "15664/15664 [==============================] - 5s 344us/step - loss: 0.1689 - acc: 0.9678 - val_loss: 0.3831 - val_acc: 0.8549\n",
      "Epoch 17/25\n",
      "15664/15664 [==============================] - 5s 339us/step - loss: 0.1502 - acc: 0.9731 - val_loss: 0.3735 - val_acc: 0.8570\n",
      "Epoch 18/25\n",
      "15664/15664 [==============================] - 6s 353us/step - loss: 0.1336 - acc: 0.9772 - val_loss: 0.3637 - val_acc: 0.8618\n",
      "Epoch 19/25\n",
      "15664/15664 [==============================] - 5s 335us/step - loss: 0.1193 - acc: 0.9791 - val_loss: 0.3588 - val_acc: 0.8600\n",
      "Epoch 20/25\n",
      "15664/15664 [==============================] - 5s 338us/step - loss: 0.1060 - acc: 0.9830 - val_loss: 0.3513 - val_acc: 0.8631\n",
      "Epoch 21/25\n",
      "15664/15664 [==============================] - 6s 357us/step - loss: 0.0944 - acc: 0.9841 - val_loss: 0.3521 - val_acc: 0.8616\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "\n",
    "def doAddNN(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"nn_eap\"] = pred_train[:,0]\n",
    "    X_train[\"nn_hpl\"] = pred_train[:,1]\n",
    "    X_train[\"nn_mws\"] = pred_train[:,2]\n",
    "    X_test[\"nn_eap\"] = pred_test[:,0]\n",
    "    X_test[\"nn_hpl\"] = pred_test[:,1]\n",
    "    X_test[\"nn_mws\"] = pred_test[:,2]\n",
    "    return X_train,X_test\n",
    "\n",
    "def doAddNN_glove(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"nn_glove_eap\"] = pred_train[:,0]\n",
    "    X_train[\"nn_glove_hpl\"] = pred_train[:,1]\n",
    "    X_train[\"nn_glove_mws\"] = pred_train[:,2]\n",
    "    X_test[\"nn_glove_eap\"] = pred_test[:,0]\n",
    "    X_test[\"nn_glove_hpl\"] = pred_test[:,1]\n",
    "    X_test[\"nn_glove_mws\"] = pred_test[:,2]\n",
    "    return X_train,X_test\n",
    "\n",
    "def doAddFastText(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"ff_eap\"] = pred_train[:,0]\n",
    "    X_train[\"ff_hpl\"] = pred_train[:,1]\n",
    "    X_train[\"ff_mws\"] = pred_train[:,2]\n",
    "    X_test[\"ff_eap\"] = pred_test[:,0]\n",
    "    X_test[\"ff_hpl\"] = pred_test[:,1]\n",
    "    X_test[\"ff_mws\"] = pred_test[:,2]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initNN(nb_words_cnt,max_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words_cnt,32,input_length=max_len))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(64,\n",
    "                     5,\n",
    "                     padding='valid',\n",
    "                     activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def initNN_glove():\n",
    "    # create a simple 3 layer sequential neural net\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(300, input_dim=100, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(300, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def doNN_glove(X_train,X_test,Y_train,xtrain_glove,xtest_glove):\n",
    "    # scale the data before any neural net:\n",
    "    scl = preprocessing.StandardScaler()\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_glove.shape[0], 3])\n",
    "    \n",
    "    for dev_index, val_index in kf.split(scl.fit_transform(xtrain_glove)):\n",
    "        dev_X, val_X = scl.fit_transform(xtrain_glove[dev_index]), scl.fit_transform(xtrain_glove[val_index])\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initNN_glove()\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=5, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(xtest_glove)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddNN_glove(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "def initFastText(embedding_dims,input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def preprocessFastText(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text\n",
    "\n",
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocessFastText(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def doFastText(X_train,X_test,Y_train):\n",
    "    min_count = 2\n",
    "\n",
    "    docs = create_docs(X_train)\n",
    "    tokenizer = Tokenizer(lower=False, filters='')\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "    maxlen = 300\n",
    "\n",
    "    docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "    input_dim = np.max(docs) + 1\n",
    "    embedding_dims = 20\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    #yvalid_enc = np_utils.to_categorical(yvalid)\n",
    "\n",
    "    docs_test = create_docs(X_test)\n",
    "    docs_test = tokenizer.texts_to_sequences(docs_test)\n",
    "    docs_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n",
    "    xtrain_pad = docs\n",
    "    xtest_pad = docs_test\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initFastText(embedding_dims,input_dim)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=25, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(docs_test)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddFastText(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "def doNN(X_train,X_test,Y_train):\n",
    "    max_len = 70\n",
    "    nb_words = 10000\n",
    "    \n",
    "    print('Processing text dataset')\n",
    "    texts_1 = []\n",
    "    for text in X_train['text']:\n",
    "        texts_1.append(text)\n",
    "\n",
    "    print('Found %s texts.' % len(texts_1))\n",
    "    test_texts_1 = []\n",
    "    for text in X_test['text']:\n",
    "        test_texts_1.append(text)\n",
    "    print('Found %s texts.' % len(test_texts_1))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=nb_words)\n",
    "    tokenizer.fit_on_texts(texts_1 + test_texts_1)\n",
    "    sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "\n",
    "    xtrain_pad = pad_sequences(sequences_1, maxlen=max_len)\n",
    "    xtest_pad = pad_sequences(test_sequences_1, maxlen=max_len)\n",
    "    del test_sequences_1\n",
    "    del sequences_1\n",
    "    nb_words_cnt = min(nb_words, len(word_index)) + 1\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    #yvalid_enc = np_utils.to_categorical(yvalid)\n",
    "\n",
    "\n",
    "\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initNN(nb_words_cnt,max_len)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=4, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(xtest_pad)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddNN(X_train,X_test,pred_train,pred_full_test/5)\n",
    "X_train,X_test = doFastText(X_train,X_test,Y_train)\n",
    "#X_train,X_test = doNN(X_train,X_test,Y_train)\n",
    "#X_train,X_test = doNN_glove(X_train,X_test,Y_train,glove_vecs_train,glove_vecs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.995657\ttest-mlogloss:0.994736\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.369774\ttest-mlogloss:0.366951\n",
      "[40]\ttrain-mlogloss:0.298115\ttest-mlogloss:0.302285\n",
      "[60]\ttrain-mlogloss:0.276498\ttest-mlogloss:0.290342\n",
      "[80]\ttrain-mlogloss:0.261833\ttest-mlogloss:0.285777\n",
      "[100]\ttrain-mlogloss:0.249429\ttest-mlogloss:0.283705\n",
      "[120]\ttrain-mlogloss:0.238555\ttest-mlogloss:0.282036\n",
      "[140]\ttrain-mlogloss:0.228883\ttest-mlogloss:0.281924\n",
      "[160]\ttrain-mlogloss:0.220208\ttest-mlogloss:0.281673\n",
      "[180]\ttrain-mlogloss:0.21151\ttest-mlogloss:0.280683\n",
      "[200]\ttrain-mlogloss:0.203784\ttest-mlogloss:0.280958\n",
      "[220]\ttrain-mlogloss:0.196543\ttest-mlogloss:0.280817\n",
      "Stopping. Best iteration:\n",
      "[182]\ttrain-mlogloss:0.210746\ttest-mlogloss:0.280609\n",
      "\n",
      "[0]\ttrain-mlogloss:0.994727\ttest-mlogloss:0.995965\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.365194\ttest-mlogloss:0.38516\n",
      "[40]\ttrain-mlogloss:0.293319\ttest-mlogloss:0.322439\n",
      "[60]\ttrain-mlogloss:0.271707\ttest-mlogloss:0.310962\n",
      "[80]\ttrain-mlogloss:0.256264\ttest-mlogloss:0.307276\n",
      "[100]\ttrain-mlogloss:0.24326\ttest-mlogloss:0.305906\n",
      "[120]\ttrain-mlogloss:0.232944\ttest-mlogloss:0.305111\n",
      "[140]\ttrain-mlogloss:0.222659\ttest-mlogloss:0.305186\n",
      "[160]\ttrain-mlogloss:0.213385\ttest-mlogloss:0.305386\n",
      "[180]\ttrain-mlogloss:0.204364\ttest-mlogloss:0.305121\n",
      "Stopping. Best iteration:\n",
      "[147]\ttrain-mlogloss:0.219219\ttest-mlogloss:0.304673\n",
      "\n",
      "[0]\ttrain-mlogloss:0.994899\ttest-mlogloss:0.998163\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.364107\ttest-mlogloss:0.387961\n",
      "[40]\ttrain-mlogloss:0.291746\ttest-mlogloss:0.324576\n",
      "[60]\ttrain-mlogloss:0.270448\ttest-mlogloss:0.313251\n",
      "[80]\ttrain-mlogloss:0.255843\ttest-mlogloss:0.308405\n",
      "[100]\ttrain-mlogloss:0.243009\ttest-mlogloss:0.306133\n",
      "[120]\ttrain-mlogloss:0.232074\ttest-mlogloss:0.304789\n",
      "[140]\ttrain-mlogloss:0.222083\ttest-mlogloss:0.304216\n",
      "[160]\ttrain-mlogloss:0.213315\ttest-mlogloss:0.303462\n",
      "[180]\ttrain-mlogloss:0.204688\ttest-mlogloss:0.304032\n",
      "[200]\ttrain-mlogloss:0.197125\ttest-mlogloss:0.303542\n",
      "Stopping. Best iteration:\n",
      "[164]\ttrain-mlogloss:0.211523\ttest-mlogloss:0.303328\n",
      "\n",
      "[0]\ttrain-mlogloss:0.995391\ttest-mlogloss:0.994169\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.368568\ttest-mlogloss:0.369002\n",
      "[40]\ttrain-mlogloss:0.296531\ttest-mlogloss:0.30733\n",
      "[60]\ttrain-mlogloss:0.27647\ttest-mlogloss:0.297176\n",
      "[80]\ttrain-mlogloss:0.262849\ttest-mlogloss:0.292989\n",
      "[100]\ttrain-mlogloss:0.251041\ttest-mlogloss:0.290022\n",
      "[120]\ttrain-mlogloss:0.240353\ttest-mlogloss:0.288858\n",
      "[140]\ttrain-mlogloss:0.230652\ttest-mlogloss:0.287142\n",
      "[160]\ttrain-mlogloss:0.22159\ttest-mlogloss:0.286494\n",
      "[180]\ttrain-mlogloss:0.213131\ttest-mlogloss:0.285649\n",
      "[200]\ttrain-mlogloss:0.204835\ttest-mlogloss:0.285348\n",
      "[220]\ttrain-mlogloss:0.197426\ttest-mlogloss:0.284683\n",
      "[240]\ttrain-mlogloss:0.190008\ttest-mlogloss:0.284712\n",
      "[260]\ttrain-mlogloss:0.182796\ttest-mlogloss:0.284897\n",
      "Stopping. Best iteration:\n",
      "[221]\ttrain-mlogloss:0.197012\ttest-mlogloss:0.284543\n",
      "\n",
      "[0]\ttrain-mlogloss:0.995255\ttest-mlogloss:0.996818\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.367449\ttest-mlogloss:0.377316\n",
      "[40]\ttrain-mlogloss:0.296138\ttest-mlogloss:0.309869\n",
      "[60]\ttrain-mlogloss:0.275539\ttest-mlogloss:0.296279\n",
      "[80]\ttrain-mlogloss:0.260967\ttest-mlogloss:0.291165\n",
      "[100]\ttrain-mlogloss:0.249522\ttest-mlogloss:0.288746\n",
      "[120]\ttrain-mlogloss:0.238363\ttest-mlogloss:0.286244\n",
      "[140]\ttrain-mlogloss:0.228537\ttest-mlogloss:0.28453\n",
      "[160]\ttrain-mlogloss:0.219739\ttest-mlogloss:0.283967\n",
      "[180]\ttrain-mlogloss:0.211344\ttest-mlogloss:0.283705\n",
      "[200]\ttrain-mlogloss:0.2035\ttest-mlogloss:0.283839\n",
      "[220]\ttrain-mlogloss:0.195673\ttest-mlogloss:0.283004\n",
      "[240]\ttrain-mlogloss:0.188325\ttest-mlogloss:0.283099\n",
      "[260]\ttrain-mlogloss:0.18148\ttest-mlogloss:0.2834\n",
      "[280]\ttrain-mlogloss:0.174946\ttest-mlogloss:0.282695\n",
      "[300]\ttrain-mlogloss:0.168888\ttest-mlogloss:0.283038\n",
      "[320]\ttrain-mlogloss:0.163181\ttest-mlogloss:0.282594\n",
      "[340]\ttrain-mlogloss:0.157646\ttest-mlogloss:0.28262\n",
      "[360]\ttrain-mlogloss:0.152447\ttest-mlogloss:0.282291\n",
      "[380]\ttrain-mlogloss:0.147234\ttest-mlogloss:0.282045\n",
      "[400]\ttrain-mlogloss:0.141827\ttest-mlogloss:0.282329\n",
      "[420]\ttrain-mlogloss:0.136932\ttest-mlogloss:0.28252\n",
      "Stopping. Best iteration:\n",
      "[379]\ttrain-mlogloss:0.147477\ttest-mlogloss:0.281996\n",
      "\n",
      "('cv scores : ', [0.28060921799118216, 0.30467332688367027, 0.30332801131823167, 0.28454333147303179, 0.28199618832421902])\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "# XGBoost\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 3\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(X_train,X_test,Y_train):\n",
    "    drop_columns=[\"id\",\"text\",\"words\",\"word_vectors\",\"sentence_vectors\"]\n",
    "    x_train = X_train.drop(drop_columns+['author'],axis=1)\n",
    "    x_test = X_test.drop(drop_columns,axis=1)\n",
    "    y_train = Y_train\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([x_train.shape[0], 3])\n",
    "    for dev_index, val_index in kf.split(x_train):\n",
    "        dev_X, val_X = x_train.loc[dev_index], x_train.loc[val_index]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, x_test, seed_val=0, colsample=0.7)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"cv scores : \", cv_scores)\n",
    "    return pred_full_test/5\n",
    "result = do(X_train,X_test,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write Results\n",
    "\n",
    "def writeResult(result,test):\n",
    "    # count number of files\n",
    "    path, dirs, files = os.walk(\"../results\").next()\n",
    "    file_count = len(files)/2+1\n",
    "\n",
    "    # Write the test results\n",
    "    data=OrderedDict()\n",
    "    data[\"id\"]=test[\"id\"] \n",
    "    data[\"EAP\"]=result[0]#[\"EAP\"]\n",
    "    data[\"HPL\"]=result[1]#[\"HPL\"]\t\n",
    "    data[\"MWS\"]=result[2]#[\"MWS\"]\n",
    "    output = pd.DataFrame(data=data)\n",
    "    filename = \"../results/result\"+str(file_count)+\".csv\"\n",
    "    output.to_csv( filename, index=False )\n",
    "    filename = \"../results/result\"+str(file_count)+\"compr.csv\"\n",
    "    output.to_csv( filename, index=False )\n",
    "    check_call(['gzip', filename])\n",
    "\n",
    "writeResult(result.T,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>words</th>\n",
       "      <th>word_vectors</th>\n",
       "      <th>sentence_vectors</th>\n",
       "      <th>sent_vec_0</th>\n",
       "      <th>sent_vec_1</th>\n",
       "      <th>sent_vec_2</th>\n",
       "      <th>sent_vec_3</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_words_nb_mws</th>\n",
       "      <th>tfidf_chars_nb_eap</th>\n",
       "      <th>tfidf_chars_nb_hpl</th>\n",
       "      <th>tfidf_chars_nb_mws</th>\n",
       "      <th>count_words_nb_eap</th>\n",
       "      <th>count_words_nb_hpl</th>\n",
       "      <th>count_words_nb_mws</th>\n",
       "      <th>count_chars_nb_eap</th>\n",
       "      <th>count_chars_nb_hpl</th>\n",
       "      <th>count_chars_nb_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, however, afforded, me, no, mea...</td>\n",
       "      <td>[[-0.20314, 0.50467, -0.25223, 0.37788, -0.686...</td>\n",
       "      <td>[-0.355503984043, 0.345234898936, 0.1080791829...</td>\n",
       "      <td>-0.004493</td>\n",
       "      <td>0.011270</td>\n",
       "      <td>0.110208</td>\n",
       "      <td>0.011623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194451</td>\n",
       "      <td>0.985953</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.011706</td>\n",
       "      <td>9.999933e-01</td>\n",
       "      <td>2.752790e-06</td>\n",
       "      <td>3.990111e-06</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.028776e-148</td>\n",
       "      <td>5.304438e-114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, once, occurred, to, me, that, the,...</td>\n",
       "      <td>[[0.13482, 0.40224, -0.42266, -0.055631, -0.55...</td>\n",
       "      <td>[-0.311601070175, 0.334749894737, 0.0187497719...</td>\n",
       "      <td>0.053792</td>\n",
       "      <td>-0.001588</td>\n",
       "      <td>0.131171</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229399</td>\n",
       "      <td>0.758384</td>\n",
       "      <td>0.117523</td>\n",
       "      <td>0.124093</td>\n",
       "      <td>8.226820e-01</td>\n",
       "      <td>1.492107e-01</td>\n",
       "      <td>2.810727e-02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.183122e-10</td>\n",
       "      <td>3.052757e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, his, left, hand, was, a, gold, snuff, box...</td>\n",
       "      <td>[[-1.0889, 0.15505, 0.31952, 0.28231, -0.26882...</td>\n",
       "      <td>[-0.352558109756, 0.408677567073, 0.1333027890...</td>\n",
       "      <td>-0.036566</td>\n",
       "      <td>0.031117</td>\n",
       "      <td>0.036176</td>\n",
       "      <td>-0.002317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178004</td>\n",
       "      <td>0.960568</td>\n",
       "      <td>0.030369</td>\n",
       "      <td>0.009062</td>\n",
       "      <td>9.999918e-01</td>\n",
       "      <td>8.206128e-06</td>\n",
       "      <td>1.064720e-08</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.399079e-66</td>\n",
       "      <td>2.669182e-142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[how, lovely, is, spring, as, we, looked, from...</td>\n",
       "      <td>[[-0.26066, -0.02226, 0.022228, -0.81433, -0.7...</td>\n",
       "      <td>[-0.418334301775, 0.342501414201, 0.1181236923...</td>\n",
       "      <td>-0.064576</td>\n",
       "      <td>0.122922</td>\n",
       "      <td>0.063774</td>\n",
       "      <td>-0.009112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546552</td>\n",
       "      <td>0.119766</td>\n",
       "      <td>0.042625</td>\n",
       "      <td>0.837609</td>\n",
       "      <td>1.436890e-09</td>\n",
       "      <td>7.472578e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.922452e-164</td>\n",
       "      <td>5.948687e-107</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[finding, nothing, else, not, even, gold, the,...</td>\n",
       "      <td>[[-0.046539, 0.61966, 0.56647, -0.46584, -1.18...</td>\n",
       "      <td>[-0.373181780822, 0.375661972603, 0.1148425356...</td>\n",
       "      <td>0.040296</td>\n",
       "      <td>0.095128</td>\n",
       "      <td>0.140202</td>\n",
       "      <td>-0.028749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242841</td>\n",
       "      <td>0.913891</td>\n",
       "      <td>0.036673</td>\n",
       "      <td>0.049436</td>\n",
       "      <td>8.960309e-01</td>\n",
       "      <td>1.016456e-01</td>\n",
       "      <td>2.323469e-03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.600385e-45</td>\n",
       "      <td>1.753184e-53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                               words  \\\n",
       "0  [this, process, however, afforded, me, no, mea...   \n",
       "1  [it, never, once, occurred, to, me, that, the,...   \n",
       "2  [in, his, left, hand, was, a, gold, snuff, box...   \n",
       "3  [how, lovely, is, spring, as, we, looked, from...   \n",
       "4  [finding, nothing, else, not, even, gold, the,...   \n",
       "\n",
       "                                        word_vectors  \\\n",
       "0  [[-0.20314, 0.50467, -0.25223, 0.37788, -0.686...   \n",
       "1  [[0.13482, 0.40224, -0.42266, -0.055631, -0.55...   \n",
       "2  [[-1.0889, 0.15505, 0.31952, 0.28231, -0.26882...   \n",
       "3  [[-0.26066, -0.02226, 0.022228, -0.81433, -0.7...   \n",
       "4  [[-0.046539, 0.61966, 0.56647, -0.46584, -1.18...   \n",
       "\n",
       "                                    sentence_vectors  sent_vec_0  sent_vec_1  \\\n",
       "0  [-0.355503984043, 0.345234898936, 0.1080791829...   -0.004493    0.011270   \n",
       "1  [-0.311601070175, 0.334749894737, 0.0187497719...    0.053792   -0.001588   \n",
       "2  [-0.352558109756, 0.408677567073, 0.1333027890...   -0.036566    0.031117   \n",
       "3  [-0.418334301775, 0.342501414201, 0.1181236923...   -0.064576    0.122922   \n",
       "4  [-0.373181780822, 0.375661972603, 0.1148425356...    0.040296    0.095128   \n",
       "\n",
       "   sent_vec_2  sent_vec_3         ...          tfidf_words_nb_mws  \\\n",
       "0    0.110208    0.011623         ...                    0.194451   \n",
       "1    0.131171    0.007966         ...                    0.229399   \n",
       "2    0.036176   -0.002317         ...                    0.178004   \n",
       "3    0.063774   -0.009112         ...                    0.546552   \n",
       "4    0.140202   -0.028749         ...                    0.242841   \n",
       "\n",
       "   tfidf_chars_nb_eap  tfidf_chars_nb_hpl  tfidf_chars_nb_mws  \\\n",
       "0            0.985953            0.002341            0.011706   \n",
       "1            0.758384            0.117523            0.124093   \n",
       "2            0.960568            0.030369            0.009062   \n",
       "3            0.119766            0.042625            0.837609   \n",
       "4            0.913891            0.036673            0.049436   \n",
       "\n",
       "   count_words_nb_eap  count_words_nb_hpl  count_words_nb_mws  \\\n",
       "0        9.999933e-01        2.752790e-06        3.990111e-06   \n",
       "1        8.226820e-01        1.492107e-01        2.810727e-02   \n",
       "2        9.999918e-01        8.206128e-06        1.064720e-08   \n",
       "3        1.436890e-09        7.472578e-10        1.000000e+00   \n",
       "4        8.960309e-01        1.016456e-01        2.323469e-03   \n",
       "\n",
       "   count_chars_nb_eap  count_chars_nb_hpl  count_chars_nb_mws  \n",
       "0        1.000000e+00       1.028776e-148       5.304438e-114  \n",
       "1        1.000000e+00        1.183122e-10        3.052757e-19  \n",
       "2        1.000000e+00        4.399079e-66       2.669182e-142  \n",
       "3       3.922452e-164       5.948687e-107        1.000000e+00  \n",
       "4        1.000000e+00        6.600385e-45        1.753184e-53  \n",
       "\n",
       "[5 rows x 134 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STOP HERE\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
