{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/16521/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import sys\n",
    "import glob\n",
    "import errno\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk.data\n",
    "import nltk\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from subprocess import check_call\n",
    "from shutil import copyfile\n",
    "from sklearn.metrics import log_loss\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "# train = \"../data/train.csv\"\n",
    "# test = \"../data/test.csv\"\n",
    "wv = \"../../../../glove.6B/glove.6B.100d.txt\"\n",
    "X_train = pd.read_csv( train, header=0,delimiter=\",\" )\n",
    "X_test = pd.read_csv( test, header=0,delimiter=\",\" )\n",
    "\n",
    "word_vecs = {}\n",
    "with open(wv) as f:\n",
    "    for line in f:\n",
    "       vals = line.split()\n",
    "       word_vecs[vals[0]] = np.array(vals[1::],dtype=float)\n",
    "authors = ['EAP','MWS','HPL']\n",
    "\n",
    "Y_train = LabelEncoder().fit_transform(X_train['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean data\n",
    "def getWordVectors(X_train,X_test,word_vecs):\n",
    "    X_train['word_vectors'] = [ [ word_vecs[word] for word in sentence if word in word_vecs] for sentence in X_train['text']]\n",
    "    X_test['word_vectors'] = [ [ word_vecs[word] for word in sentence if word in word_vecs] for sentence in X_test['text']] \n",
    "    return X_train,X_test\n",
    "\n",
    "def getSentenceVectors(X_train,X_test):\n",
    "    X_train['sentence_vectors'] =[np.mean(sentence,axis = 0) for sentence in X_train['word_vectors']]\n",
    "    X_test['sentence_vectors'] =[np.mean(sentence,axis = 0) for sentence in X_test['word_vectors']] \n",
    "    return X_train,X_test\n",
    "\n",
    "def clean(X_train,X_test):\n",
    "    X_train['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_train['text']]\n",
    "    X_test['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_test['text']]\n",
    "    return X_train,X_test\n",
    "X_train,X_test = clean(X_train,X_test)\n",
    "X_train,X_test = getWordVectors(X_train,X_test,word_vecs)\n",
    "X_train,X_test = getSentenceVectors(X_train,X_test)\n",
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Punctuation\n",
    "punctuations = [{\"id\":1,\"p\":\"[;:]\"},{\"id\":2,\"p\":\"[,.]\"},{\"id\":3,\"p\":\"[?]\"},{\"id\":4,\"p\":\"[\\']\"},{\"id\":5,\"p\":\"[\\\"]\"},{\"id\":6,\"p\":\"[;:,.?\\'\\\"]\"}]\n",
    "for p in punctuations:\n",
    "    punctuation = p[\"p\"]\n",
    "    _train =  [ sentence.split() for sentence in X_train['text'] ]\n",
    "    X_train['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _train]    \n",
    "\n",
    "    _test =  [ sentence.split() for sentence in X_test['text'] ]\n",
    "    X_test['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _test]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Stop Words\n",
    "_dist_train = [x for x in X_train['words']]\n",
    "X_train['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/len(sentence) for sentence in _dist_train]\n",
    "\n",
    "_dist_test = [x for x in X_test['words']]\n",
    "X_test['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/len(sentence) for sentence in _dist_test]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean cv score : ', 0.84221619836128525)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# tfidf - words - nb+svd\n",
    "def tfidfWords(X_train,X_test):\n",
    "    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_tfidf,test_tfidf,full_tfidf\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def runSVD(full_tfidf,train_tfidf,test_tfidf):   \n",
    "    n_comp = 20\n",
    "    svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "    svd_obj.fit(full_tfidf)\n",
    "    train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "    test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "\n",
    "    train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "    test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "    return train_svd,test_svd\n",
    "\n",
    "def do_tfidf_MNB(X_train,X_test,Y_train):\n",
    "    train_tfidf,test_tfidf,full_tfidf = tfidfWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "def do_tfidf_SVD(X_train,X_test,Y_train):\n",
    "    train_tfidf,test_tfidf,full_tfidf = tfidfWords(X_train,X_test)\n",
    "    train_svd,test_svd = runSVD(full_tfidf,train_tfidf,test_tfidf)\n",
    "    return train_svd,test_svd\n",
    "\n",
    "pred_train,pred_test = do_tfidf_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"tfidf_words_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"tfidf_words_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"tfidf_words_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"tfidf_words_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"tfidf_words_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"tfidf_words_nb_mws\"] = pred_test[:,2]\n",
    "\n",
    "# pred_train,pred_test = do_tfidf_SVD(X_train,X_test,Y_train)\n",
    "# print pred_train\n",
    "# # X_train[\"tfidf_words_nb_eap\"] = pred_train[:,0]\n",
    "# # X_train[\"tfidf_words_nb_hpl\"] = pred_train[:,1]\n",
    "# # X_train[\"tfidf_words_nb_mws\"] = pred_train[:,2]\n",
    "# # X_test[\"tfidf_words_nb_eap\"] = pred_test[:,0]\n",
    "# # X_test[\"tfidf_words_nb_hpl\"] = pred_test[:,1]\n",
    "# # X_test[\"tfidf_words_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean cv score : ', 0.7904152589474216)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# tfidf - chars - nb+svd\n",
    "def tfidfWords(X_train,X_test):\n",
    "    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,5),analyzer='char')\n",
    "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_tfidf,test_tfidf\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(X_train,X_test,Y_train):\n",
    "    train_tfidf,test_tfidf = tfidfWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "pred_train,pred_test = do(X_train,X_test,Y_train)\n",
    "X_train[\"tfidf_chars_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"tfidf_chars_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"tfidf_chars_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"tfidf_chars_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"tfidf_chars_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"tfidf_chars_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean cv score : ', 0.45091841616567468)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# count - words - nb\n",
    "def countWords(X_train,X_test):\n",
    "    count_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_count = count_vec.transform(X_train['text'].values.tolist())\n",
    "    test_count = count_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_count,test_count\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_count_MNB(X_train,X_test,Y_train):\n",
    "    train_count,test_count=countWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "pred_train,pred_test = do_count_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"count_words_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"count_words_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"count_words_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"count_words_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"count_words_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"count_words_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean cv score : ', 3.750763922681903)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# count - chars - nb\n",
    "def countChars(X_train,X_test):\n",
    "    count_vec = CountVectorizer(ngram_range=(1,7),analyzer='char')\n",
    "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_count = count_vec.transform(X_train['text'].values.tolist())\n",
    "    test_count = count_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_count,test_count\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_count_chars_MNB(X_train,X_test,Y_train):\n",
    "    train_count,test_count=countChars(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "pred_train,pred_test = do_count_chars_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"count_chars_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"count_chars_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"count_chars_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"count_chars_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"count_chars_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"count_chars_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word Embeddings\n",
    "# this function creates a normalized vector for the whole sentence\n",
    "#X_train['sentence_vectors'][0]\n",
    "\n",
    "#foo.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
    "X_train = pd.concat([X_train,pd.DataFrame(X_train['sentence_vectors'].tolist())],axis=1)\n",
    "X_test = pd.concat([X_test,pd.DataFrame(X_test['sentence_vectors'].tolist())],axis=1)\n",
    "\n",
    "# xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "# xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]\n",
    "# xtrain_glove = np.array(xtrain_glove)\n",
    "# xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "def loadWordVecs():\n",
    "    embeddings_index = {}\n",
    "    f = open(wv)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def sent2vec(embeddings_index,s): # this function creates a normalized vector for the whole sentence\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stopwords.words('english')]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "def doGlove(x_train,x_test):\n",
    "    embeddings_index = loadWordVecs()\n",
    "    # create sentence vectors using the above function for training and validation set\n",
    "    xtrain_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_train)]\n",
    "    xtest_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_test)]\n",
    "    xtrain_glove = np.array(xtrain_glove)\n",
    "    xtest_glove = np.array(xtest_glove)\n",
    "    return xtrain_glove,xtest_glove,embeddings_index\n",
    "\n",
    "glove_vecs_train,glove_vecs_test,embeddings_index = doGlove(X_train['text'],X_test['text'])\n",
    "X_train[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_train.tolist())\n",
    "X_test[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_test.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29451/29451 [00:00<00:00, 329204.58it/s]\n",
      "/Users/16521/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(40, 3, padding=\"same\", activation=\"relu\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/5\n",
      "15663/15663 [==============================] - 15s 944us/step - loss: 1.0777 - val_loss: 1.0545\n",
      "Epoch 2/5\n",
      "15663/15663 [==============================] - 5s 339us/step - loss: 0.8259 - val_loss: 0.5658\n",
      "Epoch 3/5\n",
      "15663/15663 [==============================] - 5s 345us/step - loss: 0.4621 - val_loss: 0.4317\n",
      "Epoch 4/5\n",
      "15663/15663 [==============================] - 5s 344us/step - loss: 0.3087 - val_loss: 0.4116\n",
      "Epoch 5/5\n",
      "15663/15663 [==============================] - 5s 348us/step - loss: 0.2276 - val_loss: 0.4149\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/5\n",
      "15663/15663 [==============================] - 15s 952us/step - loss: 1.0763 - val_loss: 1.0227\n",
      "Epoch 2/5\n",
      "15663/15663 [==============================] - 5s 340us/step - loss: 0.7522 - val_loss: 0.5439\n",
      "Epoch 3/5\n",
      "15663/15663 [==============================] - 6s 353us/step - loss: 0.4208 - val_loss: 0.4500\n",
      "Epoch 4/5\n",
      "15663/15663 [==============================] - 5s 346us/step - loss: 0.2932 - val_loss: 0.4080\n",
      "Epoch 5/5\n",
      "15663/15663 [==============================] - 5s 346us/step - loss: 0.2116 - val_loss: 0.4163\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/5\n",
      "15663/15663 [==============================] - 15s 966us/step - loss: 1.0782 - val_loss: 1.0550\n",
      "Epoch 2/5\n",
      "15663/15663 [==============================] - 5s 341us/step - loss: 0.8724 - val_loss: 0.6644\n",
      "Epoch 3/5\n",
      "15663/15663 [==============================] - 6s 353us/step - loss: 0.5010 - val_loss: 0.4693\n",
      "Epoch 4/5\n",
      "15663/15663 [==============================] - 5s 340us/step - loss: 0.3085 - val_loss: 0.4373\n",
      "Epoch 5/5\n",
      "15663/15663 [==============================] - 5s 345us/step - loss: 0.2285 - val_loss: 0.4306\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/5\n",
      "15663/15663 [==============================] - 15s 976us/step - loss: 1.0766 - val_loss: 1.0248\n",
      "Epoch 2/5\n",
      "15663/15663 [==============================] - 5s 345us/step - loss: 0.7739 - val_loss: 0.5229\n",
      "Epoch 3/5\n",
      "15663/15663 [==============================] - 6s 353us/step - loss: 0.4339 - val_loss: 0.4293\n",
      "Epoch 4/5\n",
      "15663/15663 [==============================] - 6s 371us/step - loss: 0.3018 - val_loss: 0.3969\n",
      "Epoch 5/5\n",
      "15663/15663 [==============================] - 6s 358us/step - loss: 0.2204 - val_loss: 0.4009\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/5\n",
      "15664/15664 [==============================] - 16s 1ms/step - loss: 1.0805 - val_loss: 1.0468\n",
      "Epoch 2/5\n",
      "15664/15664 [==============================] - 6s 361us/step - loss: 0.9048 - val_loss: 0.7639\n",
      "Epoch 3/5\n",
      "15664/15664 [==============================] - 5s 342us/step - loss: 0.5800 - val_loss: 0.4926\n",
      "Epoch 4/5\n",
      "15664/15664 [==============================] - 5s 342us/step - loss: 0.3553 - val_loss: 0.4294\n",
      "Epoch 5/5\n",
      "15664/15664 [==============================] - 5s 337us/step - loss: 0.2569 - val_loss: 0.4066\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "\n",
    "def doAddNN(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"nn_eap\"] = pred_train[:,0]\n",
    "    X_train[\"nn_hpl\"] = pred_train[:,1]\n",
    "    X_train[\"nn_mws\"] = pred_train[:,2]\n",
    "    X_test[\"nn_eap\"] = pred_test[:,0]\n",
    "    X_test[\"nn_hpl\"] = pred_test[:,1]\n",
    "    X_test[\"nn_mws\"] = pred_test[:,2]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initNN():\n",
    "    # create a simple 3 layer sequential neural net\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(40, input_dim=50, activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(40, activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def initCNN(word_index,embedding_matrix,max_len):\n",
    "    # A simple CNN with glove embeddings and two dense layers\n",
    "    model = Sequential([\n",
    "        Embedding(len(word_index) + 1,30,input_length=max_len),\n",
    "        Dropout(0.5),\n",
    "        Conv1D(40, 3, border_mode='same', activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(40, activation='relu'),\n",
    "        Dropout(0.6),\n",
    "        Dense(3, activation='softmax')])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def doNN(X_train,X_test):\n",
    "    #glove_vecs_train,glove_vecs_test = doGlove(X_train['text'],X_test['text'])\n",
    "    # scale the data before any neural net:\n",
    "    token = text.Tokenizer(num_words=None)\n",
    "    max_len = 70\n",
    "\n",
    "    token.fit_on_texts(list(X_train['text']) + list(X_test['text']))\n",
    "    xtrain_seq = token.texts_to_sequences(X_train['text'])\n",
    "    xtest_seq = token.texts_to_sequences(X_test['text'])\n",
    "\n",
    "    # zero pad the sequences\n",
    "    xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "    xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
    "\n",
    "    word_index = token.word_index\n",
    "\n",
    "    # create an embedding matrix for the words we have in the dataset\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    #yvalid_enc = np_utils.to_categorical(yvalid)\n",
    "\n",
    "\n",
    "\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initCNN(word_index,embedding_matrix,max_len)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=64, epochs=5, verbose=1,validation_data=(val_X, val_y))\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(xtest_pad)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddNN(X_train,X_test,pred_train,pred_full_test/5)\n",
    "X_train,X_test = doNN(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.048079044, 0.94079792, 0.011123078)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.999628\ttest-mlogloss:0.999482\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.391298\ttest-mlogloss:0.397139\n",
      "[40]\ttrain-mlogloss:0.319198\ttest-mlogloss:0.333202\n",
      "[60]\ttrain-mlogloss:0.296807\ttest-mlogloss:0.320042\n",
      "[80]\ttrain-mlogloss:0.281649\ttest-mlogloss:0.314684\n",
      "[100]\ttrain-mlogloss:0.26805\ttest-mlogloss:0.311341\n",
      "[120]\ttrain-mlogloss:0.257358\ttest-mlogloss:0.309619\n",
      "[140]\ttrain-mlogloss:0.247042\ttest-mlogloss:0.308337\n",
      "[160]\ttrain-mlogloss:0.237471\ttest-mlogloss:0.307778\n",
      "[180]\ttrain-mlogloss:0.229021\ttest-mlogloss:0.306946\n",
      "[200]\ttrain-mlogloss:0.220467\ttest-mlogloss:0.306314\n",
      "[220]\ttrain-mlogloss:0.21224\ttest-mlogloss:0.306209\n",
      "[240]\ttrain-mlogloss:0.2049\ttest-mlogloss:0.305659\n",
      "[260]\ttrain-mlogloss:0.198042\ttest-mlogloss:0.306052\n",
      "[280]\ttrain-mlogloss:0.191251\ttest-mlogloss:0.306039\n",
      "[300]\ttrain-mlogloss:0.185274\ttest-mlogloss:0.306677\n",
      "Stopping. Best iteration:\n",
      "[268]\ttrain-mlogloss:0.195345\ttest-mlogloss:0.305629\n",
      "\n",
      "[0]\ttrain-mlogloss:0.999526\ttest-mlogloss:1.00018\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.39023\ttest-mlogloss:0.402977\n",
      "[40]\ttrain-mlogloss:0.317533\ttest-mlogloss:0.338536\n",
      "[60]\ttrain-mlogloss:0.294737\ttest-mlogloss:0.326095\n",
      "[80]\ttrain-mlogloss:0.278318\ttest-mlogloss:0.320218\n",
      "[100]\ttrain-mlogloss:0.264799\ttest-mlogloss:0.317107\n",
      "[120]\ttrain-mlogloss:0.253136\ttest-mlogloss:0.315331\n",
      "[140]\ttrain-mlogloss:0.243449\ttest-mlogloss:0.314412\n",
      "[160]\ttrain-mlogloss:0.233866\ttest-mlogloss:0.313322\n",
      "[180]\ttrain-mlogloss:0.224785\ttest-mlogloss:0.313335\n",
      "[200]\ttrain-mlogloss:0.216126\ttest-mlogloss:0.313503\n",
      "[220]\ttrain-mlogloss:0.208551\ttest-mlogloss:0.313949\n",
      "Stopping. Best iteration:\n",
      "[184]\ttrain-mlogloss:0.222945\ttest-mlogloss:0.313156\n",
      "\n",
      "[0]\ttrain-mlogloss:0.9991\ttest-mlogloss:1.00116\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.388778\ttest-mlogloss:0.408823\n",
      "[40]\ttrain-mlogloss:0.31727\ttest-mlogloss:0.346024\n",
      "[60]\ttrain-mlogloss:0.294981\ttest-mlogloss:0.332711\n",
      "[80]\ttrain-mlogloss:0.278959\ttest-mlogloss:0.32651\n",
      "[100]\ttrain-mlogloss:0.265731\ttest-mlogloss:0.323559\n",
      "[120]\ttrain-mlogloss:0.254268\ttest-mlogloss:0.3217\n",
      "[140]\ttrain-mlogloss:0.244204\ttest-mlogloss:0.320387\n",
      "[160]\ttrain-mlogloss:0.23475\ttest-mlogloss:0.320079\n",
      "[180]\ttrain-mlogloss:0.225922\ttest-mlogloss:0.320117\n",
      "[200]\ttrain-mlogloss:0.217925\ttest-mlogloss:0.319824\n",
      "[220]\ttrain-mlogloss:0.210555\ttest-mlogloss:0.319857\n",
      "[240]\ttrain-mlogloss:0.203172\ttest-mlogloss:0.319896\n",
      "[260]\ttrain-mlogloss:0.196712\ttest-mlogloss:0.319405\n",
      "[280]\ttrain-mlogloss:0.190351\ttest-mlogloss:0.319626\n",
      "[300]\ttrain-mlogloss:0.183795\ttest-mlogloss:0.319813\n",
      "Stopping. Best iteration:\n",
      "[263]\ttrain-mlogloss:0.19578\ttest-mlogloss:0.319223\n",
      "\n",
      "[0]\ttrain-mlogloss:0.999459\ttest-mlogloss:0.999521\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.392946\ttest-mlogloss:0.393677\n",
      "[40]\ttrain-mlogloss:0.323025\ttest-mlogloss:0.33018\n",
      "[60]\ttrain-mlogloss:0.302547\ttest-mlogloss:0.316721\n",
      "[80]\ttrain-mlogloss:0.28818\ttest-mlogloss:0.310516\n",
      "[100]\ttrain-mlogloss:0.277373\ttest-mlogloss:0.306719\n",
      "[120]\ttrain-mlogloss:0.267964\ttest-mlogloss:0.304472\n",
      "[140]\ttrain-mlogloss:0.257887\ttest-mlogloss:0.301933\n",
      "[160]\ttrain-mlogloss:0.248486\ttest-mlogloss:0.300659\n",
      "[180]\ttrain-mlogloss:0.239843\ttest-mlogloss:0.29978\n",
      "[200]\ttrain-mlogloss:0.231165\ttest-mlogloss:0.298912\n",
      "[220]\ttrain-mlogloss:0.223378\ttest-mlogloss:0.298285\n",
      "[240]\ttrain-mlogloss:0.216187\ttest-mlogloss:0.297938\n",
      "[260]\ttrain-mlogloss:0.209454\ttest-mlogloss:0.297434\n",
      "[280]\ttrain-mlogloss:0.202826\ttest-mlogloss:0.297024\n",
      "[300]\ttrain-mlogloss:0.195958\ttest-mlogloss:0.296908\n",
      "[320]\ttrain-mlogloss:0.189197\ttest-mlogloss:0.296801\n",
      "[340]\ttrain-mlogloss:0.183426\ttest-mlogloss:0.296977\n",
      "[360]\ttrain-mlogloss:0.177098\ttest-mlogloss:0.297284\n",
      "Stopping. Best iteration:\n",
      "[321]\ttrain-mlogloss:0.188814\ttest-mlogloss:0.29667\n",
      "\n",
      "[0]\ttrain-mlogloss:1.00139\ttest-mlogloss:1.00188\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.392521\ttest-mlogloss:0.398125\n",
      "[40]\ttrain-mlogloss:0.320924\ttest-mlogloss:0.332228\n",
      "[60]\ttrain-mlogloss:0.299434\ttest-mlogloss:0.318579\n",
      "[80]\ttrain-mlogloss:0.284693\ttest-mlogloss:0.312198\n",
      "[100]\ttrain-mlogloss:0.271956\ttest-mlogloss:0.307989\n",
      "[120]\ttrain-mlogloss:0.260447\ttest-mlogloss:0.30659\n",
      "[140]\ttrain-mlogloss:0.250379\ttest-mlogloss:0.304197\n",
      "[160]\ttrain-mlogloss:0.2411\ttest-mlogloss:0.303807\n",
      "[180]\ttrain-mlogloss:0.232305\ttest-mlogloss:0.302508\n",
      "[200]\ttrain-mlogloss:0.223931\ttest-mlogloss:0.302046\n",
      "[220]\ttrain-mlogloss:0.216092\ttest-mlogloss:0.301299\n",
      "[240]\ttrain-mlogloss:0.208862\ttest-mlogloss:0.301176\n",
      "[260]\ttrain-mlogloss:0.201433\ttest-mlogloss:0.300911\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "# XGBoost\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 3\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(X_train,X_test,Y_train):\n",
    "    drop_columns=[\"id\",\"text\",\"words\",\"word_vectors\",\"sentence_vectors\"]\n",
    "    x_train = X_train.drop(drop_columns+['author'],axis=1)\n",
    "    x_test = X_test.drop(drop_columns,axis=1)\n",
    "    y_train = Y_train\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([x_train.shape[0], 3])\n",
    "    for dev_index, val_index in kf.split(x_train):\n",
    "        dev_X, val_X = x_train.loc[dev_index], x_train.loc[val_index]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, x_test, seed_val=0, colsample=0.7)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"cv scores : \", cv_scores)\n",
    "    return pred_full_test/5\n",
    "result = do(X_train,X_test,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write Results\n",
    "\n",
    "def writeResult(result,test):\n",
    "    # count number of files\n",
    "    path, dirs, files = os.walk(\"../results\").next()\n",
    "    file_count = len(files)/2+1\n",
    "\n",
    "    # Write the test results\n",
    "    data=OrderedDict()\n",
    "    data[\"id\"]=test[\"id\"] \n",
    "    data[\"EAP\"]=result[0]#[\"EAP\"]\n",
    "    data[\"HPL\"]=result[1]#[\"HPL\"]\t\n",
    "    data[\"MWS\"]=result[2]#[\"MWS\"]\n",
    "    output = pd.DataFrame(data=data)\n",
    "    filename = \"../results/result\"+str(file_count)+\".csv\"\n",
    "    output.to_csv( filename, index=False )\n",
    "    filename = \"../results/result\"+str(file_count)+\"compr.csv\"\n",
    "    output.to_csv( filename, index=False )\n",
    "    check_call(['gzip', filename])\n",
    "\n",
    "writeResult(result.T,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'text', u'author', u'words', u'word_vectors',\n",
       "       u'sentence_vectors', u'punc_1', u'punc_2', u'punc_3', u'punc_4',\n",
       "       ...\n",
       "       u'sent_vec_290', u'sent_vec_291', u'sent_vec_292', u'sent_vec_293',\n",
       "       u'sent_vec_294', u'sent_vec_295', u'sent_vec_296', u'sent_vec_297',\n",
       "       u'sent_vec_298', u'sent_vec_299'],\n",
       "      dtype='object', length=328)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'text', u'author', u'words', u'word_vectors',\n",
       "       u'sentence_vectors', u'sent_vec_0', u'sent_vec_1', u'sent_vec_2',\n",
       "       u'sent_vec_3', u'sent_vec_4', u'sent_vec_5', u'sent_vec_6',\n",
       "       u'sent_vec_7', u'sent_vec_8', u'sent_vec_9', u'sent_vec_10',\n",
       "       u'sent_vec_11', u'sent_vec_12', u'sent_vec_13', u'sent_vec_14',\n",
       "       u'sent_vec_15', u'sent_vec_16', u'sent_vec_17', u'sent_vec_18',\n",
       "       u'sent_vec_19', u'sent_vec_20', u'sent_vec_21', u'sent_vec_22',\n",
       "       u'sent_vec_23', u'sent_vec_24', u'sent_vec_25', u'sent_vec_26',\n",
       "       u'sent_vec_27', u'sent_vec_28', u'sent_vec_29', u'sent_vec_30',\n",
       "       u'sent_vec_31', u'sent_vec_32', u'sent_vec_33', u'sent_vec_34',\n",
       "       u'sent_vec_35', u'sent_vec_36', u'sent_vec_37', u'sent_vec_38',\n",
       "       u'sent_vec_39', u'sent_vec_40', u'sent_vec_41', u'sent_vec_42',\n",
       "       u'sent_vec_43', u'sent_vec_44', u'sent_vec_45', u'sent_vec_46',\n",
       "       u'sent_vec_47', u'sent_vec_48', u'sent_vec_49', u'nn_eap', u'nn_hpl',\n",
       "       u'nn_mws', u'punc_1', u'punc_2', u'punc_3', u'punc_4', u'punc_5',\n",
       "       u'punc_6', u'stop_word', u'tfidf_words_nb_eap', u'tfidf_words_nb_hpl',\n",
       "       u'tfidf_words_nb_mws', u'tfidf_chars_nb_eap', u'tfidf_chars_nb_hpl',\n",
       "       u'tfidf_chars_nb_mws', u'count_words_nb_eap', u'count_words_nb_hpl',\n",
       "       u'count_words_nb_mws', u'count_chars_nb_eap', u'count_chars_nb_hpl',\n",
       "       u'count_chars_nb_mws'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>words</th>\n",
       "      <th>word_vectors</th>\n",
       "      <th>sentence_vectors</th>\n",
       "      <th>nn_eap</th>\n",
       "      <th>nn_hpl</th>\n",
       "      <th>nn_mws</th>\n",
       "      <th>punc_1</th>\n",
       "      <th>...</th>\n",
       "      <th>sent_vec_90</th>\n",
       "      <th>sent_vec_91</th>\n",
       "      <th>sent_vec_92</th>\n",
       "      <th>sent_vec_93</th>\n",
       "      <th>sent_vec_94</th>\n",
       "      <th>sent_vec_95</th>\n",
       "      <th>sent_vec_96</th>\n",
       "      <th>sent_vec_97</th>\n",
       "      <th>sent_vec_98</th>\n",
       "      <th>sent_vec_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, however, afforded, me, no, mea...</td>\n",
       "      <td>[[-0.22701, 0.70329, 0.96125, 0.93479, 0.7205,...</td>\n",
       "      <td>[-0.102682611702, 0.843608723404, 0.6941906914...</td>\n",
       "      <td>0.996464</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>4.878049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064199</td>\n",
       "      <td>-0.051707</td>\n",
       "      <td>-0.032212</td>\n",
       "      <td>-0.015696</td>\n",
       "      <td>-0.039146</td>\n",
       "      <td>-0.034641</td>\n",
       "      <td>-0.026215</td>\n",
       "      <td>-0.079093</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>0.062398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, once, occurred, to, me, that, the,...</td>\n",
       "      <td>[[-0.37915, 0.61848, 0.9593, 0.90403, 0.36806,...</td>\n",
       "      <td>[-0.071579877193, 0.818990877193, 0.7487534035...</td>\n",
       "      <td>0.747714</td>\n",
       "      <td>0.200467</td>\n",
       "      <td>0.051819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042077</td>\n",
       "      <td>0.016603</td>\n",
       "      <td>-0.056828</td>\n",
       "      <td>0.019743</td>\n",
       "      <td>-0.083541</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>-0.020753</td>\n",
       "      <td>-0.027814</td>\n",
       "      <td>0.011478</td>\n",
       "      <td>0.054295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, his, left, hand, was, a, gold, snuff, box...</td>\n",
       "      <td>[[-0.27004, 1.1144, 1.0493, 0.57924, 0.78968, ...</td>\n",
       "      <td>[-0.137265402439, 0.810834207317, 0.6727634268...</td>\n",
       "      <td>0.849465</td>\n",
       "      <td>0.114169</td>\n",
       "      <td>0.036367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065868</td>\n",
       "      <td>-0.024816</td>\n",
       "      <td>-0.029570</td>\n",
       "      <td>0.050657</td>\n",
       "      <td>-0.152419</td>\n",
       "      <td>-0.024303</td>\n",
       "      <td>-0.033989</td>\n",
       "      <td>-0.062024</td>\n",
       "      <td>0.138996</td>\n",
       "      <td>0.046701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[how, lovely, is, spring, as, we, looked, from...</td>\n",
       "      <td>[[-0.043861, 1.3183, -0.03715, 0.85478, 0.1221...</td>\n",
       "      <td>[-0.0491552662722, 0.823676804734, 0.681447236...</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.994305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>-0.109733</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.046515</td>\n",
       "      <td>-0.238110</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>-0.143425</td>\n",
       "      <td>0.060398</td>\n",
       "      <td>0.095089</td>\n",
       "      <td>0.036426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[finding, nothing, else, not, even, gold, the,...</td>\n",
       "      <td>[[0.11891, 0.15255, -0.082073, -0.74144, 0.759...</td>\n",
       "      <td>[-0.0283006027397, 0.804272054795, 0.685995212...</td>\n",
       "      <td>0.104045</td>\n",
       "      <td>0.710142</td>\n",
       "      <td>0.185813</td>\n",
       "      <td>3.703704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049627</td>\n",
       "      <td>-0.031102</td>\n",
       "      <td>0.043574</td>\n",
       "      <td>-0.036604</td>\n",
       "      <td>-0.119386</td>\n",
       "      <td>-0.016236</td>\n",
       "      <td>-0.116524</td>\n",
       "      <td>-0.016949</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>0.084453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                               words  \\\n",
       "0  [this, process, however, afforded, me, no, mea...   \n",
       "1  [it, never, once, occurred, to, me, that, the,...   \n",
       "2  [in, his, left, hand, was, a, gold, snuff, box...   \n",
       "3  [how, lovely, is, spring, as, we, looked, from...   \n",
       "4  [finding, nothing, else, not, even, gold, the,...   \n",
       "\n",
       "                                        word_vectors  \\\n",
       "0  [[-0.22701, 0.70329, 0.96125, 0.93479, 0.7205,...   \n",
       "1  [[-0.37915, 0.61848, 0.9593, 0.90403, 0.36806,...   \n",
       "2  [[-0.27004, 1.1144, 1.0493, 0.57924, 0.78968, ...   \n",
       "3  [[-0.043861, 1.3183, -0.03715, 0.85478, 0.1221...   \n",
       "4  [[0.11891, 0.15255, -0.082073, -0.74144, 0.759...   \n",
       "\n",
       "                                    sentence_vectors    nn_eap    nn_hpl  \\\n",
       "0  [-0.102682611702, 0.843608723404, 0.6941906914...  0.996464  0.002704   \n",
       "1  [-0.071579877193, 0.818990877193, 0.7487534035...  0.747714  0.200467   \n",
       "2  [-0.137265402439, 0.810834207317, 0.6727634268...  0.849465  0.114169   \n",
       "3  [-0.0491552662722, 0.823676804734, 0.681447236...  0.002261  0.003433   \n",
       "4  [-0.0283006027397, 0.804272054795, 0.685995212...  0.104045  0.710142   \n",
       "\n",
       "     nn_mws    punc_1     ...       sent_vec_90  sent_vec_91  sent_vec_92  \\\n",
       "0  0.000832  4.878049     ...         -0.064199    -0.051707    -0.032212   \n",
       "1  0.051819  0.000000     ...         -0.042077     0.016603    -0.056828   \n",
       "2  0.036367  0.000000     ...         -0.065868    -0.024816    -0.029570   \n",
       "3  0.994305  0.000000     ...          0.014403    -0.109733     0.002690   \n",
       "4  0.185813  3.703704     ...          0.049627    -0.031102     0.043574   \n",
       "\n",
       "   sent_vec_93  sent_vec_94  sent_vec_95  sent_vec_96  sent_vec_97  \\\n",
       "0    -0.015696    -0.039146    -0.034641    -0.026215    -0.079093   \n",
       "1     0.019743    -0.083541     0.018332    -0.020753    -0.027814   \n",
       "2     0.050657    -0.152419    -0.024303    -0.033989    -0.062024   \n",
       "3     0.046515    -0.238110     0.006557    -0.143425     0.060398   \n",
       "4    -0.036604    -0.119386    -0.016236    -0.116524    -0.016949   \n",
       "\n",
       "   sent_vec_98  sent_vec_99  \n",
       "0     0.068413     0.062398  \n",
       "1     0.011478     0.054295  \n",
       "2     0.138996     0.046701  \n",
       "3     0.095089     0.036426  \n",
       "4     0.043204     0.084453  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['sent_vec_'+str(i) for i in range(300)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00833853,  0.0029562 ,  1.48870525])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
