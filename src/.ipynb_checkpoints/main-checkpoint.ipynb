{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import sys\n",
    "import glob\n",
    "import errno\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk.data\n",
    "import nltk\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from subprocess import check_call\n",
    "from shutil import copyfile\n",
    "from sklearn.metrics import log_loss\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import Merge,Lambda,Input,GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,TimeDistributed\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from sklearn.linear_model import SGDClassifier as sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "train = \"../data/train.csv\"\n",
    "test = \"../data/test.csv\"\n",
    "wv = \"../../../../glove.6B/glove.6B.100d.txt\"\n",
    "X_train = pd.read_csv( train, header=0,delimiter=\",\" )\n",
    "X_test = pd.read_csv( test, header=0,delimiter=\",\" )\n",
    "\n",
    "word_vecs = {}\n",
    "with open(wv) as f:\n",
    "    for line in f:\n",
    "       vals = line.split()\n",
    "       word_vecs[vals[0]] = np.array(vals[1::],dtype=float)\n",
    "authors = ['EAP','MWS','HPL']\n",
    "\n",
    "Y_train = LabelEncoder().fit_transform(X_train['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean data\n",
    "def getWordVectors(X_train,X_test,word_vecs):\n",
    "    X_train['word_vectors'] = [ [ word_vecs[word] for word in sentence if word in word_vecs] for sentence in X_train['text']]\n",
    "    X_test['word_vectors'] = [ [ word_vecs[word] for word in sentence if word in word_vecs] for sentence in X_test['text']] \n",
    "    return X_train,X_test\n",
    "\n",
    "def getSentenceVectors(X_train,X_test):\n",
    "    X_train['sentence_vectors'] =[np.mean(sentence,axis = 0) for sentence in X_train['word_vectors']]\n",
    "    X_test['sentence_vectors'] =[np.mean(sentence,axis = 0) for sentence in X_test['word_vectors']] \n",
    "    return X_train,X_test\n",
    "\n",
    "def clean(X_train,X_test):\n",
    "    X_train['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_train['text']]\n",
    "    X_test['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_test['text']]\n",
    "    return X_train,X_test\n",
    "X_train,X_test = clean(X_train,X_test)\n",
    "X_train,X_test = getWordVectors(X_train,X_test,word_vecs)\n",
    "X_train,X_test = getSentenceVectors(X_train,X_test)\n",
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Punctuation\n",
    "punctuations = [{\"id\":1,\"p\":\"[;:]\"},{\"id\":2,\"p\":\"[,.]\"},{\"id\":3,\"p\":\"[?]\"},{\"id\":4,\"p\":\"[\\']\"},{\"id\":5,\"p\":\"[\\\"]\"},{\"id\":6,\"p\":\"[;:,.?\\'\\\"]\"}]\n",
    "for p in punctuations:\n",
    "    punctuation = p[\"p\"]\n",
    "    _train =  [ sentence.split() for sentence in X_train['text'] ]\n",
    "    X_train['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _train]    \n",
    "\n",
    "    _test =  [ sentence.split() for sentence in X_test['text'] ]\n",
    "    X_test['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _test]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Stop Words\n",
    "_dist_train = [x for x in X_train['words']]\n",
    "X_train['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/len(sentence) for sentence in _dist_train]\n",
    "\n",
    "_dist_test = [x for x in X_test['words']]\n",
    "X_test['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/len(sentence) for sentence in _dist_test]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean cv score : ', 0.84221619836128525)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# tfidf - words - nb+svd\n",
    "def tfidfWords(X_train,X_test):\n",
    "    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_tfidf,test_tfidf,full_tfidf\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def runSVD(full_tfidf,train_tfidf,test_tfidf):   \n",
    "    n_comp = 20\n",
    "    svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "    svd_obj.fit(full_tfidf)\n",
    "    train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "    test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "\n",
    "    train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "    test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "    return train_svd,test_svd\n",
    "\n",
    "def do_tfidf_MNB(X_train,X_test,Y_train):\n",
    "    train_tfidf,test_tfidf,full_tfidf = tfidfWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "def do_tfidf_SVD(X_train,X_test,Y_train):\n",
    "    train_tfidf,test_tfidf,full_tfidf = tfidfWords(X_train,X_test)\n",
    "    train_svd,test_svd = runSVD(full_tfidf,train_tfidf,test_tfidf)\n",
    "    return train_svd,test_svd\n",
    "\n",
    "pred_train,pred_test = do_tfidf_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"tfidf_words_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"tfidf_words_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"tfidf_words_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"tfidf_words_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"tfidf_words_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"tfidf_words_nb_mws\"] = pred_test[:,2]\n",
    "\n",
    "# pred_train,pred_test = do_tfidf_SVD(X_train,X_test,Y_train)\n",
    "# print pred_train\n",
    "# # X_train[\"tfidf_words_nb_eap\"] = pred_train[:,0]\n",
    "# # X_train[\"tfidf_words_nb_hpl\"] = pred_train[:,1]\n",
    "# # X_train[\"tfidf_words_nb_mws\"] = pred_train[:,2]\n",
    "# # X_test[\"tfidf_words_nb_eap\"] = pred_test[:,0]\n",
    "# # X_test[\"tfidf_words_nb_hpl\"] = pred_test[:,1]\n",
    "# # X_test[\"tfidf_words_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean cv score : ', 0.7904152589474216)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# tfidf - chars - nb+svd\n",
    "def tfidfWords(X_train,X_test):\n",
    "    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,5),analyzer='char')\n",
    "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_tfidf,test_tfidf\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(X_train,X_test,Y_train):\n",
    "    train_tfidf,test_tfidf = tfidfWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "pred_train,pred_test = do(X_train,X_test,Y_train)\n",
    "X_train[\"tfidf_chars_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"tfidf_chars_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"tfidf_chars_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"tfidf_chars_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"tfidf_chars_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"tfidf_chars_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean cv score : ', 0.45091841616567468)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# count - words - nb\n",
    "def countWords(X_train,X_test):\n",
    "    count_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_count = count_vec.transform(X_train['text'].values.tolist())\n",
    "    test_count = count_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_count,test_count\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_count_MNB(X_train,X_test,Y_train):\n",
    "    train_count,test_count=countWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "pred_train,pred_test = do_count_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"count_words_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"count_words_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"count_words_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"count_words_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"count_words_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"count_words_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean cv score : ', 3.750763922681903)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# count - chars - nb\n",
    "def countChars(X_train,X_test):\n",
    "    count_vec = CountVectorizer(ngram_range=(1,7),analyzer='char')\n",
    "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_count = count_vec.transform(X_train['text'].values.tolist())\n",
    "    test_count = count_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_count,test_count\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_count_chars_MNB(X_train,X_test,Y_train):\n",
    "    train_count,test_count=countChars(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "pred_train,pred_test = do_count_chars_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"count_chars_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"count_chars_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"count_chars_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"count_chars_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"count_chars_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"count_chars_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/19579 [00:00<06:10, 52.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19579/19579 [01:50<00:00, 177.82it/s]\n",
      "100%|██████████| 8392/8392 [00:46<00:00, 179.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "def loadWordVecs():\n",
    "    embeddings_index = {}\n",
    "    f = open(wv)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def sent2vec(embeddings_index,s): # this function creates a normalized vector for the whole sentence\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stopwords.words('english')]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "def doGlove(x_train,x_test):\n",
    "    embeddings_index = loadWordVecs()\n",
    "    # create sentence vectors using the above function for training and validation set\n",
    "    xtrain_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_train)]\n",
    "    xtest_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_test)]\n",
    "    xtrain_glove = np.array(xtrain_glove)\n",
    "    xtest_glove = np.array(xtest_glove)\n",
    "    return xtrain_glove,xtest_glove,embeddings_index\n",
    "\n",
    "glove_vecs_train,glove_vecs_test,embeddings_index = doGlove(X_train['text'],X_test['text'])\n",
    "X_train[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_train.tolist())\n",
    "X_test[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_test.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29451/29451 [00:00<00:00, 312497.34it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_141_input to have shape (None, 100) but got array with shape (15663, 70)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-516b2b89366f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mpred_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_val_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoAddNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_full_test\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-96-516b2b89366f>\u001b[0m in \u001b[0;36mdoNN\u001b[0;34m(X_train, X_test, embeddings_index)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mytrain_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdev_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mpred_val_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mpred_test_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/16521/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/16521/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1572\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1574\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1575\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/16521/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1405\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1408\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1409\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/16521/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_141_input to have shape (None, 100) but got array with shape (15663, 70)"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "\n",
    "def doAddNN(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"nn_eap\"] = pred_train[:,0]\n",
    "    X_train[\"nn_hpl\"] = pred_train[:,1]\n",
    "    X_train[\"nn_mws\"] = pred_train[:,2]\n",
    "    X_test[\"nn_eap\"] = pred_test[:,0]\n",
    "    X_test[\"nn_hpl\"] = pred_test[:,1]\n",
    "    X_test[\"nn_mws\"] = pred_test[:,2]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initNN():\n",
    "    # create a simple 3 layer sequential neural net\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1024, input_dim=50, activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def initCNN(word_index,embedding_matrix,max_len):\n",
    "    units = 10 # Number of nodes in the Dense layers\n",
    "    dropout = 0.5 # Percentage of nodes to drop\n",
    "    nb_filter = 3 # Number of filters to use in Convolution1D\n",
    "    filter_length = 50 # Length of filter for Convolution1D\n",
    "    # Initialize weights and biases for the Dense layers\n",
    "    weights = initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=2)\n",
    "    bias = bias_initializer='zeros'\n",
    "\n",
    "    model = Sequential()\n",
    "#     model.add(Embedding(len(word_index) + 1,\n",
    "#                          100,\n",
    "#                          weights = [embedding_matrix],\n",
    "#                          input_length = max_len,\n",
    "#                          trainable = False))\n",
    "    model.add(Dense(1024, input_dim=100, activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Dense(1024, activation='relu'))\n",
    "#     model.add(Dropout(0.6))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "#     model1.add(Conv1D(filters = nb_filter, \n",
    "#                              kernel_size = filter_length, \n",
    "#                              padding = 'same'))\n",
    "#     model1.add(BatchNormalization())\n",
    "#     model1.add(Activation('relu'))\n",
    "#     model1.add(Dropout(dropout))\n",
    "\n",
    "# #     model1.add(Conv1D(filters = nb_filter, \n",
    "# #                              kernel_size = filter_length, \n",
    "# #                              padding = 'same'))\n",
    "# #     model1.add(BatchNormalization())\n",
    "# #     model1.add(Activation('relu'))\n",
    "# #     model1.add(Dropout(dropout))\n",
    "    \n",
    "    \n",
    "#     model1.add(Flatten())\n",
    "#     model1.add(Dense(3))\n",
    "#     model1.add(Activation('softmax'))\n",
    "    \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def doNN(X_train,X_test,embeddings_index):\n",
    "    #glove_vecs_train,glove_vecs_test = doGlove(X_train['text'],X_test['text'])\n",
    "    # scale the data before any neural net:\n",
    "    token = text.Tokenizer(num_words=None)\n",
    "    max_len = 70\n",
    "\n",
    "    token.fit_on_texts(list(X_train['text']) + list(X_test['text']))\n",
    "    xtrain_seq = token.texts_to_sequences(X_train['text'])\n",
    "    xtest_seq = token.texts_to_sequences(X_test['text'])\n",
    "\n",
    "    # zero pad the sequences\n",
    "    xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "    xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
    "\n",
    "    word_index = token.word_index\n",
    "\n",
    "    # create an embedding matrix for the words we have in the dataset\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    #yvalid_enc = np_utils.to_categorical(yvalid)\n",
    "\n",
    "\n",
    "\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initCNN(word_index,embedding_matrix,max_len)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=100, verbose=1,validation_data=(val_X, val_y))\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(xtest_pad)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddNN(X_train,X_test,pred_train,pred_full_test/5)\n",
    "X_train,X_test = doNN(X_train,X_test,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
    "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
    "    inputs: shape=[None, num_vectors, dim_vector]\n",
    "    output: shape=[None, num_vectors]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "\n",
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
    "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
    "    masked Tensor.\n",
    "    For example:\n",
    "        ```\n",
    "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
    "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
    "        out = Mask()(x)  # out.shape=[8, 6]\n",
    "        # or\n",
    "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
    "        ```\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
    "            # compute lengths of capsules\n",
    "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
    "            # generate the mask which is a one-hot code.\n",
    "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
    "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
    "\n",
    "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
    "        # mask.shape=[None, num_capsule]\n",
    "        # masked.shape=[None, num_capsule * dim_capsule]\n",
    "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
    "        return masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # no true label provided\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])\n",
    "\n",
    "\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
    "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routing: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
    "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "        inputs_expand = K.expand_dims(inputs, 1)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
    "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
    "        # Regard the first two dimensions as `batch` dimension,\n",
    "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
    "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "        inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
    "\n",
    "        \"\"\"\n",
    "        # Begin: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        b = K.zeros(shape=[self.batch_size, self.num_capsule, self.input_num_capsule])\n",
    "        def body(i, b, outputs):\n",
    "            c = tf.nn.softmax(b, dim=1)  # dim=2 is the num_capsule dimension\n",
    "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))\n",
    "            if i != 1:\n",
    "                b = b + K.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "            return [i-1, b, outputs]\n",
    "        cond = lambda i, b, inputs_hat: i > 0\n",
    "        loop_vars = [K.constant(self.num_routing), b, K.sum(inputs_hat, 2, keepdims=False)]\n",
    "        shape_invariants = [tf.TensorShape([]),\n",
    "                            tf.TensorShape([None, self.num_capsule, self.input_num_capsule]),\n",
    "                            tf.TensorShape([None, self.num_capsule, self.dim_capsule])]\n",
    "        _, _, outputs = tf.while_loop(cond, body, loop_vars, shape_invariants)\n",
    "        # End: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
    "        # In forward pass, `inputs_hat_stopped` = `inputs_hat`;\n",
    "        # In backward, no gradient can flow from `inputs_hat_stopped` back to `inputs_hat`.\n",
    "        inputs_hat_stopped = K.stop_gradient(inputs_hat)\n",
    "        \n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
    "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
    "\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "            c = tf.nn.softmax(b, dim=1)\n",
    "\n",
    "            # At last iteration, use `inputs_hat` to compute `outputs` in order to backpropagate gradient\n",
    "            if i == self.num_routing - 1:\n",
    "                # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
    "                # outputs.shape=[None, num_capsule, dim_capsule]\n",
    "                outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
    "            else:  # Otherwise, use `inputs_hat_stopped` to update `b`. No gradients flow on this path.\n",
    "                outputs = squash(K.batch_dot(c, inputs_hat_stopped, [2, 2]))\n",
    "\n",
    "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
    "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "                b += K.batch_dot(outputs, inputs_hat_stopped, [2, 3])\n",
    "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
    "\n",
    "\n",
    "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_capsule: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv2d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Keras Data\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'return_sequences')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-06638ca6fd82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mytrain_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdev_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mn_stem_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdev_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_stem_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mpred_val_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-156-06638ca6fd82>\u001b[0m in \u001b[0;36minitModel\u001b[0;34m(n_stem_seq, maxlen)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     emb_lstm = SpatialDropout1D(emb_dropout_rate,) (Embedding(n_stem_seq, embed_dim\n\u001b[0;32m---> 50\u001b[0;31m                                                 \u001b[0;34m,\u001b[0m\u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                                                                ) (input_text))\n\u001b[1;32m     52\u001b[0m     \u001b[0mdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0memb_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/16521/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/16521/anaconda2/lib/python2.7/site-packages/keras/layers/embeddings.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint, mask_zero, input_length, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_shape'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/16521/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Keyword argument not understood:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'return_sequences')"
     ]
    }
   ],
   "source": [
    "# Test - magic-embeddings-keras-a-toy-example\n",
    "\n",
    "#STEMMING WORDS\n",
    "import nltk.stem as stm\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Dropout, Embedding\n",
    "from keras.layers import Flatten, Input, SpatialDropout1D, Reshape\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "    \n",
    "\n",
    "def processText(X_train,X_test):\n",
    "    #PROCESS TEXT: RAW\n",
    "    stemmer = stm.SnowballStemmer(\"english\")\n",
    "    X_train[\"stem_text\"] = X_train.text.apply(lambda x: (\" \").join([stemmer.stem(z) for z in re.sub(\"[^a-zA-Z0-9]\",\" \", x).split(\" \")]))\n",
    "    X_test[\"stem_text\"] = X_test.text.apply(lambda x: (\" \").join([stemmer.stem(z) for z in re.sub(\"[^a-zA-Z0-9]\",\" \", x).split(\" \")]))\n",
    "\n",
    "    tok_raw = Tokenizer()\n",
    "    tok_raw.fit_on_texts(X_train.text.str.lower())\n",
    "    tok_stem = Tokenizer()\n",
    "    tok_stem.fit_on_texts(X_train.stem_text)\n",
    "    X_train[\"seq_text_stem\"] = tok_stem.texts_to_sequences(X_train.stem_text)\n",
    "    X_test[\"seq_text_stem\"] = tok_stem.texts_to_sequences(X_test.stem_text)\n",
    "    return X_train,X_test\n",
    "\n",
    "#EXTRACT DATA FOR KERAS MODEL\n",
    "def get_keras_data(dataset, maxlen):\n",
    "    return pad_sequences(dataset.seq_text_stem, maxlen=maxlen)\n",
    "\n",
    "# Model\n",
    "def doAddModel(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"nn_eap\"] = pred_train[:,0]\n",
    "    X_train[\"nn_hpl\"] = pred_train[:,1]\n",
    "    X_train[\"nn_mws\"] = pred_train[:,2]\n",
    "    X_test[\"nn_eap\"] = pred_test[:,0]\n",
    "    X_test[\"nn_hpl\"] = pred_test[:,1]\n",
    "    X_test[\"nn_mws\"] = pred_test[:,2]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initModel(n_stem_seq,maxlen):\n",
    "    embed_dim = 50\n",
    "    dropout_rate = 0.9\n",
    "    emb_dropout_rate = 0.9\n",
    "\n",
    "    input_text = Input(shape=[maxlen], name=\"stem_input\")\n",
    "\n",
    "    emb_lstm = SpatialDropout1D(emb_dropout_rate,) (Embedding(n_stem_seq, embed_dim\n",
    "                                                ,input_length = maxlen\n",
    "                                                               ) (input_text))\n",
    "    dense = Dropout(dropout_rate) (Dense(1024) (Flatten() (emb_lstm)))\n",
    "    dense = Reshape((128, 8)) (dense)\n",
    "    dense = Flatten() (Mask()(CapsuleLayer(128, 8)(dense)))\n",
    "\n",
    "    output = Dense(3, activation=\"softmax\")(dense)\n",
    "\n",
    "    model = Model([input_text], output)\n",
    "\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "#def doModel(X_train,X_test):\n",
    "#X_train,X_test = processText(X_train,X_test)\n",
    "maxlen = 70\n",
    "print \"Getting Keras Data\\n\"\n",
    "stem_train = get_keras_data(X_train,maxlen)\n",
    "stem_test = get_keras_data(X_test,maxlen)\n",
    "\n",
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "#yvalid_enc = np_utils.to_categorical(yvalid)\n",
    "\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([X_train.shape[0], 3])\n",
    "for dev_index, val_index in kf.split(X_train):\n",
    "    dev_X, val_X = stem_train[dev_index], stem_train[val_index]\n",
    "    dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "    n_stem_seq = np.max( [np.max(stem_train[dev_index]), np.max(stem_train[val_index])])+1\n",
    "    model = initModel(n_stem_seq,maxlen)\n",
    "    model.fit(dev_X, y=dev_y, batch_size=1024, epochs=100, verbose=1,validation_data=(val_X, val_y))\n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(stem_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "#return doAddModel(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "X_train,X_test = doModel(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>sent_vec_0</th>\n",
       "      <th>sent_vec_1</th>\n",
       "      <th>sent_vec_2</th>\n",
       "      <th>sent_vec_3</th>\n",
       "      <th>sent_vec_4</th>\n",
       "      <th>sent_vec_5</th>\n",
       "      <th>sent_vec_6</th>\n",
       "      <th>...</th>\n",
       "      <th>sent_vec_92</th>\n",
       "      <th>sent_vec_93</th>\n",
       "      <th>sent_vec_94</th>\n",
       "      <th>sent_vec_95</th>\n",
       "      <th>sent_vec_96</th>\n",
       "      <th>sent_vec_97</th>\n",
       "      <th>sent_vec_98</th>\n",
       "      <th>sent_vec_99</th>\n",
       "      <th>stem_text</th>\n",
       "      <th>seq_text_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>-0.004493</td>\n",
       "      <td>0.011270</td>\n",
       "      <td>0.110208</td>\n",
       "      <td>0.011623</td>\n",
       "      <td>-0.044851</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>-0.034797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032212</td>\n",
       "      <td>-0.015696</td>\n",
       "      <td>-0.039146</td>\n",
       "      <td>-0.034641</td>\n",
       "      <td>-0.026215</td>\n",
       "      <td>-0.079093</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>0.062398</td>\n",
       "      <td>this process  howev  afford me no mean of asce...</td>\n",
       "      <td>[27, 1876, 161, 743, 22, 37, 201, 2, 1652, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>0.053792</td>\n",
       "      <td>-0.001588</td>\n",
       "      <td>0.131171</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>-0.123774</td>\n",
       "      <td>0.076791</td>\n",
       "      <td>0.056179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056828</td>\n",
       "      <td>0.019743</td>\n",
       "      <td>-0.083541</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>-0.020753</td>\n",
       "      <td>-0.027814</td>\n",
       "      <td>0.011478</td>\n",
       "      <td>0.054295</td>\n",
       "      <td>it never onc occur to me that the fumbl might ...</td>\n",
       "      <td>[10, 99, 138, 672, 4, 22, 9, 1, 3675, 85, 23, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>-0.036566</td>\n",
       "      <td>0.031117</td>\n",
       "      <td>0.036176</td>\n",
       "      <td>-0.002317</td>\n",
       "      <td>0.015307</td>\n",
       "      <td>0.039265</td>\n",
       "      <td>-0.082055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029570</td>\n",
       "      <td>0.050657</td>\n",
       "      <td>-0.152419</td>\n",
       "      <td>-0.024303</td>\n",
       "      <td>-0.033989</td>\n",
       "      <td>-0.062024</td>\n",
       "      <td>0.138996</td>\n",
       "      <td>0.046701</td>\n",
       "      <td>in his left hand was a gold snuff box  from wh...</td>\n",
       "      <td>[7, 15, 164, 122, 8, 6, 935, 4166, 636, 24, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>-0.064576</td>\n",
       "      <td>0.122922</td>\n",
       "      <td>0.063774</td>\n",
       "      <td>-0.009112</td>\n",
       "      <td>-0.012569</td>\n",
       "      <td>0.140672</td>\n",
       "      <td>-0.011279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.046515</td>\n",
       "      <td>-0.238110</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>-0.143425</td>\n",
       "      <td>0.060398</td>\n",
       "      <td>0.095089</td>\n",
       "      <td>0.036426</td>\n",
       "      <td>how love is spring as we look from windsor ter...</td>\n",
       "      <td>[133, 106, 26, 749, 16, 35, 94, 24, 903, 2393,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>0.040296</td>\n",
       "      <td>0.095128</td>\n",
       "      <td>0.140202</td>\n",
       "      <td>-0.028749</td>\n",
       "      <td>-0.001092</td>\n",
       "      <td>0.065856</td>\n",
       "      <td>-0.090490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043574</td>\n",
       "      <td>-0.036604</td>\n",
       "      <td>-0.119386</td>\n",
       "      <td>-0.016236</td>\n",
       "      <td>-0.116524</td>\n",
       "      <td>-0.016949</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>0.084453</td>\n",
       "      <td>find noth els  not even gold  the superintend ...</td>\n",
       "      <td>[207, 194, 882, 20, 67, 935, 1, 3204, 1281, 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   sent_vec_0  sent_vec_1  sent_vec_2  sent_vec_3  sent_vec_4  sent_vec_5  \\\n",
       "0   -0.004493    0.011270    0.110208    0.011623   -0.044851    0.038956   \n",
       "1    0.053792   -0.001588    0.131171    0.007966   -0.123774    0.076791   \n",
       "2   -0.036566    0.031117    0.036176   -0.002317    0.015307    0.039265   \n",
       "3   -0.064576    0.122922    0.063774   -0.009112   -0.012569    0.140672   \n",
       "4    0.040296    0.095128    0.140202   -0.028749   -0.001092    0.065856   \n",
       "\n",
       "   sent_vec_6                        ...                          sent_vec_92  \\\n",
       "0   -0.034797                        ...                            -0.032212   \n",
       "1    0.056179                        ...                            -0.056828   \n",
       "2   -0.082055                        ...                            -0.029570   \n",
       "3   -0.011279                        ...                             0.002690   \n",
       "4   -0.090490                        ...                             0.043574   \n",
       "\n",
       "   sent_vec_93  sent_vec_94  sent_vec_95  sent_vec_96  sent_vec_97  \\\n",
       "0    -0.015696    -0.039146    -0.034641    -0.026215    -0.079093   \n",
       "1     0.019743    -0.083541     0.018332    -0.020753    -0.027814   \n",
       "2     0.050657    -0.152419    -0.024303    -0.033989    -0.062024   \n",
       "3     0.046515    -0.238110     0.006557    -0.143425     0.060398   \n",
       "4    -0.036604    -0.119386    -0.016236    -0.116524    -0.016949   \n",
       "\n",
       "   sent_vec_98  sent_vec_99  \\\n",
       "0     0.068413     0.062398   \n",
       "1     0.011478     0.054295   \n",
       "2     0.138996     0.046701   \n",
       "3     0.095089     0.036426   \n",
       "4     0.043204     0.084453   \n",
       "\n",
       "                                           stem_text  \\\n",
       "0  this process  howev  afford me no mean of asce...   \n",
       "1  it never onc occur to me that the fumbl might ...   \n",
       "2  in his left hand was a gold snuff box  from wh...   \n",
       "3  how love is spring as we look from windsor ter...   \n",
       "4  find noth els  not even gold  the superintend ...   \n",
       "\n",
       "                                       seq_text_stem  \n",
       "0  [27, 1876, 161, 743, 22, 37, 201, 2, 1652, 1, ...  \n",
       "1  [10, 99, 138, 672, 4, 22, 9, 1, 3675, 85, 23, ...  \n",
       "2  [7, 15, 164, 122, 8, 6, 935, 4166, 636, 24, 18...  \n",
       "3  [133, 106, 26, 749, 16, 35, 94, 24, 903, 2393,...  \n",
       "4  [207, 194, 882, 20, 67, 935, 1, 3204, 1281, 15...  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.999628\ttest-mlogloss:0.999482\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.391298\ttest-mlogloss:0.397139\n",
      "[40]\ttrain-mlogloss:0.319198\ttest-mlogloss:0.333202\n",
      "[60]\ttrain-mlogloss:0.296807\ttest-mlogloss:0.320042\n",
      "[80]\ttrain-mlogloss:0.281649\ttest-mlogloss:0.314684\n",
      "[100]\ttrain-mlogloss:0.26805\ttest-mlogloss:0.311341\n",
      "[120]\ttrain-mlogloss:0.257358\ttest-mlogloss:0.309619\n",
      "[140]\ttrain-mlogloss:0.247042\ttest-mlogloss:0.308337\n",
      "[160]\ttrain-mlogloss:0.237471\ttest-mlogloss:0.307778\n",
      "[180]\ttrain-mlogloss:0.229021\ttest-mlogloss:0.306946\n",
      "[200]\ttrain-mlogloss:0.220467\ttest-mlogloss:0.306314\n",
      "[220]\ttrain-mlogloss:0.21224\ttest-mlogloss:0.306209\n",
      "[240]\ttrain-mlogloss:0.2049\ttest-mlogloss:0.305659\n",
      "[260]\ttrain-mlogloss:0.198042\ttest-mlogloss:0.306052\n",
      "[280]\ttrain-mlogloss:0.191251\ttest-mlogloss:0.306039\n",
      "[300]\ttrain-mlogloss:0.185274\ttest-mlogloss:0.306677\n",
      "Stopping. Best iteration:\n",
      "[268]\ttrain-mlogloss:0.195345\ttest-mlogloss:0.305629\n",
      "\n",
      "[0]\ttrain-mlogloss:0.999526\ttest-mlogloss:1.00018\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.39023\ttest-mlogloss:0.402977\n",
      "[40]\ttrain-mlogloss:0.317533\ttest-mlogloss:0.338536\n",
      "[60]\ttrain-mlogloss:0.294737\ttest-mlogloss:0.326095\n",
      "[80]\ttrain-mlogloss:0.278318\ttest-mlogloss:0.320218\n",
      "[100]\ttrain-mlogloss:0.264799\ttest-mlogloss:0.317107\n",
      "[120]\ttrain-mlogloss:0.253136\ttest-mlogloss:0.315331\n",
      "[140]\ttrain-mlogloss:0.243449\ttest-mlogloss:0.314412\n",
      "[160]\ttrain-mlogloss:0.233866\ttest-mlogloss:0.313322\n",
      "[180]\ttrain-mlogloss:0.224785\ttest-mlogloss:0.313335\n",
      "[200]\ttrain-mlogloss:0.216126\ttest-mlogloss:0.313503\n",
      "[220]\ttrain-mlogloss:0.208551\ttest-mlogloss:0.313949\n",
      "Stopping. Best iteration:\n",
      "[184]\ttrain-mlogloss:0.222945\ttest-mlogloss:0.313156\n",
      "\n",
      "[0]\ttrain-mlogloss:0.9991\ttest-mlogloss:1.00116\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.388778\ttest-mlogloss:0.408823\n",
      "[40]\ttrain-mlogloss:0.31727\ttest-mlogloss:0.346024\n",
      "[60]\ttrain-mlogloss:0.294981\ttest-mlogloss:0.332711\n",
      "[80]\ttrain-mlogloss:0.278959\ttest-mlogloss:0.32651\n",
      "[100]\ttrain-mlogloss:0.265731\ttest-mlogloss:0.323559\n",
      "[120]\ttrain-mlogloss:0.254268\ttest-mlogloss:0.3217\n",
      "[140]\ttrain-mlogloss:0.244204\ttest-mlogloss:0.320387\n",
      "[160]\ttrain-mlogloss:0.23475\ttest-mlogloss:0.320079\n",
      "[180]\ttrain-mlogloss:0.225922\ttest-mlogloss:0.320117\n",
      "[200]\ttrain-mlogloss:0.217925\ttest-mlogloss:0.319824\n",
      "[220]\ttrain-mlogloss:0.210555\ttest-mlogloss:0.319857\n",
      "[240]\ttrain-mlogloss:0.203172\ttest-mlogloss:0.319896\n",
      "[260]\ttrain-mlogloss:0.196712\ttest-mlogloss:0.319405\n",
      "[280]\ttrain-mlogloss:0.190351\ttest-mlogloss:0.319626\n",
      "[300]\ttrain-mlogloss:0.183795\ttest-mlogloss:0.319813\n",
      "Stopping. Best iteration:\n",
      "[263]\ttrain-mlogloss:0.19578\ttest-mlogloss:0.319223\n",
      "\n",
      "[0]\ttrain-mlogloss:0.999459\ttest-mlogloss:0.999521\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.392946\ttest-mlogloss:0.393677\n",
      "[40]\ttrain-mlogloss:0.323025\ttest-mlogloss:0.33018\n",
      "[60]\ttrain-mlogloss:0.302547\ttest-mlogloss:0.316721\n",
      "[80]\ttrain-mlogloss:0.28818\ttest-mlogloss:0.310516\n",
      "[100]\ttrain-mlogloss:0.277373\ttest-mlogloss:0.306719\n",
      "[120]\ttrain-mlogloss:0.267964\ttest-mlogloss:0.304472\n",
      "[140]\ttrain-mlogloss:0.257887\ttest-mlogloss:0.301933\n",
      "[160]\ttrain-mlogloss:0.248486\ttest-mlogloss:0.300659\n",
      "[180]\ttrain-mlogloss:0.239843\ttest-mlogloss:0.29978\n",
      "[200]\ttrain-mlogloss:0.231165\ttest-mlogloss:0.298912\n",
      "[220]\ttrain-mlogloss:0.223378\ttest-mlogloss:0.298285\n",
      "[240]\ttrain-mlogloss:0.216187\ttest-mlogloss:0.297938\n",
      "[260]\ttrain-mlogloss:0.209454\ttest-mlogloss:0.297434\n",
      "[280]\ttrain-mlogloss:0.202826\ttest-mlogloss:0.297024\n",
      "[300]\ttrain-mlogloss:0.195958\ttest-mlogloss:0.296908\n",
      "[320]\ttrain-mlogloss:0.189197\ttest-mlogloss:0.296801\n",
      "[340]\ttrain-mlogloss:0.183426\ttest-mlogloss:0.296977\n",
      "[360]\ttrain-mlogloss:0.177098\ttest-mlogloss:0.297284\n",
      "Stopping. Best iteration:\n",
      "[321]\ttrain-mlogloss:0.188814\ttest-mlogloss:0.29667\n",
      "\n",
      "[0]\ttrain-mlogloss:1.00139\ttest-mlogloss:1.00188\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.392521\ttest-mlogloss:0.398125\n",
      "[40]\ttrain-mlogloss:0.320924\ttest-mlogloss:0.332228\n",
      "[60]\ttrain-mlogloss:0.299434\ttest-mlogloss:0.318579\n",
      "[80]\ttrain-mlogloss:0.284693\ttest-mlogloss:0.312198\n",
      "[100]\ttrain-mlogloss:0.271956\ttest-mlogloss:0.307989\n",
      "[120]\ttrain-mlogloss:0.260447\ttest-mlogloss:0.30659\n",
      "[140]\ttrain-mlogloss:0.250379\ttest-mlogloss:0.304197\n",
      "[160]\ttrain-mlogloss:0.2411\ttest-mlogloss:0.303807\n",
      "[180]\ttrain-mlogloss:0.232305\ttest-mlogloss:0.302508\n",
      "[200]\ttrain-mlogloss:0.223931\ttest-mlogloss:0.302046\n",
      "[220]\ttrain-mlogloss:0.216092\ttest-mlogloss:0.301299\n",
      "[240]\ttrain-mlogloss:0.208862\ttest-mlogloss:0.301176\n",
      "[260]\ttrain-mlogloss:0.201433\ttest-mlogloss:0.300911\n",
      "[280]\ttrain-mlogloss:0.194984\ttest-mlogloss:0.300713\n",
      "[300]\ttrain-mlogloss:0.18832\ttest-mlogloss:0.300567\n",
      "[320]\ttrain-mlogloss:0.18174\ttest-mlogloss:0.299995\n",
      "[340]\ttrain-mlogloss:0.175883\ttest-mlogloss:0.300525\n",
      "[360]\ttrain-mlogloss:0.170178\ttest-mlogloss:0.30097\n",
      "Stopping. Best iteration:\n",
      "[311]\ttrain-mlogloss:0.184725\ttest-mlogloss:0.299855\n",
      "\n",
      "('cv scores : ', [0.3056285535172279, 0.31315569257832282, 0.31922331437350299, 0.29667029711805509, 0.2998551036757951])\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "# XGBoost\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 3\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(X_train,X_test,Y_train):\n",
    "    drop_columns=[\"id\",\"text\",\"words\",\"word_vectors\",\"sentence_vectors\"]\n",
    "    x_train = X_train.drop(drop_columns+['author'],axis=1)\n",
    "    x_test = X_test.drop(drop_columns,axis=1)\n",
    "    y_train = Y_train\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([x_train.shape[0], 3])\n",
    "    for dev_index, val_index in kf.split(x_train):\n",
    "        dev_X, val_X = x_train.loc[dev_index], x_train.loc[val_index]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, x_test, seed_val=0, colsample=0.7)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"cv scores : \", cv_scores)\n",
    "    return pred_full_test/5\n",
    "result = do(X_train,X_test,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write Results\n",
    "\n",
    "def writeResult(result,test):\n",
    "    # count number of files\n",
    "    path, dirs, files = os.walk(\"../results\").next()\n",
    "    file_count = len(files)/2+1\n",
    "\n",
    "    # Write the test results\n",
    "    data=OrderedDict()\n",
    "    data[\"id\"]=test[\"id\"] \n",
    "    data[\"EAP\"]=result[0]#[\"EAP\"]\n",
    "    data[\"HPL\"]=result[1]#[\"HPL\"]\t\n",
    "    data[\"MWS\"]=result[2]#[\"MWS\"]\n",
    "    output = pd.DataFrame(data=data)\n",
    "    filename = \"../results/result\"+str(file_count)+\".csv\"\n",
    "    output.to_csv( filename, index=False )\n",
    "    filename = \"../results/result\"+str(file_count)+\"compr.csv\"\n",
    "    output.to_csv( filename, index=False )\n",
    "    check_call(['gzip', filename])\n",
    "\n",
    "writeResult(result.T,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'text', u'author', u'words', u'word_vectors',\n",
       "       u'sentence_vectors', u'sent_vec_0', u'sent_vec_1', u'sent_vec_2',\n",
       "       u'sent_vec_3', u'sent_vec_4', u'sent_vec_5', u'sent_vec_6',\n",
       "       u'sent_vec_7', u'sent_vec_8', u'sent_vec_9', u'sent_vec_10',\n",
       "       u'sent_vec_11', u'sent_vec_12', u'sent_vec_13', u'sent_vec_14',\n",
       "       u'sent_vec_15', u'sent_vec_16', u'sent_vec_17', u'sent_vec_18',\n",
       "       u'sent_vec_19', u'sent_vec_20', u'sent_vec_21', u'sent_vec_22',\n",
       "       u'sent_vec_23', u'sent_vec_24', u'sent_vec_25', u'sent_vec_26',\n",
       "       u'sent_vec_27', u'sent_vec_28', u'sent_vec_29', u'sent_vec_30',\n",
       "       u'sent_vec_31', u'sent_vec_32', u'sent_vec_33', u'sent_vec_34',\n",
       "       u'sent_vec_35', u'sent_vec_36', u'sent_vec_37', u'sent_vec_38',\n",
       "       u'sent_vec_39', u'sent_vec_40', u'sent_vec_41', u'sent_vec_42',\n",
       "       u'sent_vec_43', u'sent_vec_44', u'sent_vec_45', u'sent_vec_46',\n",
       "       u'sent_vec_47', u'sent_vec_48', u'sent_vec_49', u'nn_eap', u'nn_hpl',\n",
       "       u'nn_mws', u'punc_1', u'punc_2', u'punc_3', u'punc_4', u'punc_5',\n",
       "       u'punc_6', u'stop_word', u'tfidf_words_nb_eap', u'tfidf_words_nb_hpl',\n",
       "       u'tfidf_words_nb_mws', u'tfidf_chars_nb_eap', u'tfidf_chars_nb_hpl',\n",
       "       u'tfidf_chars_nb_mws', u'count_words_nb_eap', u'count_words_nb_hpl',\n",
       "       u'count_words_nb_mws', u'count_chars_nb_eap', u'count_chars_nb_hpl',\n",
       "       u'count_chars_nb_mws'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>sent_vec_0</th>\n",
       "      <th>sent_vec_1</th>\n",
       "      <th>sent_vec_2</th>\n",
       "      <th>sent_vec_3</th>\n",
       "      <th>sent_vec_4</th>\n",
       "      <th>sent_vec_5</th>\n",
       "      <th>sent_vec_6</th>\n",
       "      <th>...</th>\n",
       "      <th>sent_vec_90</th>\n",
       "      <th>sent_vec_91</th>\n",
       "      <th>sent_vec_92</th>\n",
       "      <th>sent_vec_93</th>\n",
       "      <th>sent_vec_94</th>\n",
       "      <th>sent_vec_95</th>\n",
       "      <th>sent_vec_96</th>\n",
       "      <th>sent_vec_97</th>\n",
       "      <th>sent_vec_98</th>\n",
       "      <th>sent_vec_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>-0.004493</td>\n",
       "      <td>0.011270</td>\n",
       "      <td>0.110208</td>\n",
       "      <td>0.011623</td>\n",
       "      <td>-0.044851</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>-0.034797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064199</td>\n",
       "      <td>-0.051707</td>\n",
       "      <td>-0.032212</td>\n",
       "      <td>-0.015696</td>\n",
       "      <td>-0.039146</td>\n",
       "      <td>-0.034641</td>\n",
       "      <td>-0.026215</td>\n",
       "      <td>-0.079093</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>0.062398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>0.053792</td>\n",
       "      <td>-0.001588</td>\n",
       "      <td>0.131171</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>-0.123774</td>\n",
       "      <td>0.076791</td>\n",
       "      <td>0.056179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042077</td>\n",
       "      <td>0.016603</td>\n",
       "      <td>-0.056828</td>\n",
       "      <td>0.019743</td>\n",
       "      <td>-0.083541</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>-0.020753</td>\n",
       "      <td>-0.027814</td>\n",
       "      <td>0.011478</td>\n",
       "      <td>0.054295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>-0.036566</td>\n",
       "      <td>0.031117</td>\n",
       "      <td>0.036176</td>\n",
       "      <td>-0.002317</td>\n",
       "      <td>0.015307</td>\n",
       "      <td>0.039265</td>\n",
       "      <td>-0.082055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065868</td>\n",
       "      <td>-0.024816</td>\n",
       "      <td>-0.029570</td>\n",
       "      <td>0.050657</td>\n",
       "      <td>-0.152419</td>\n",
       "      <td>-0.024303</td>\n",
       "      <td>-0.033989</td>\n",
       "      <td>-0.062024</td>\n",
       "      <td>0.138996</td>\n",
       "      <td>0.046701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>-0.064576</td>\n",
       "      <td>0.122922</td>\n",
       "      <td>0.063774</td>\n",
       "      <td>-0.009112</td>\n",
       "      <td>-0.012569</td>\n",
       "      <td>0.140672</td>\n",
       "      <td>-0.011279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>-0.109733</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.046515</td>\n",
       "      <td>-0.238110</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>-0.143425</td>\n",
       "      <td>0.060398</td>\n",
       "      <td>0.095089</td>\n",
       "      <td>0.036426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>0.040296</td>\n",
       "      <td>0.095128</td>\n",
       "      <td>0.140202</td>\n",
       "      <td>-0.028749</td>\n",
       "      <td>-0.001092</td>\n",
       "      <td>0.065856</td>\n",
       "      <td>-0.090490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049627</td>\n",
       "      <td>-0.031102</td>\n",
       "      <td>0.043574</td>\n",
       "      <td>-0.036604</td>\n",
       "      <td>-0.119386</td>\n",
       "      <td>-0.016236</td>\n",
       "      <td>-0.116524</td>\n",
       "      <td>-0.016949</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>0.084453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   sent_vec_0  sent_vec_1  sent_vec_2  sent_vec_3  sent_vec_4  sent_vec_5  \\\n",
       "0   -0.004493    0.011270    0.110208    0.011623   -0.044851    0.038956   \n",
       "1    0.053792   -0.001588    0.131171    0.007966   -0.123774    0.076791   \n",
       "2   -0.036566    0.031117    0.036176   -0.002317    0.015307    0.039265   \n",
       "3   -0.064576    0.122922    0.063774   -0.009112   -0.012569    0.140672   \n",
       "4    0.040296    0.095128    0.140202   -0.028749   -0.001092    0.065856   \n",
       "\n",
       "   sent_vec_6     ...       sent_vec_90  sent_vec_91  sent_vec_92  \\\n",
       "0   -0.034797     ...         -0.064199    -0.051707    -0.032212   \n",
       "1    0.056179     ...         -0.042077     0.016603    -0.056828   \n",
       "2   -0.082055     ...         -0.065868    -0.024816    -0.029570   \n",
       "3   -0.011279     ...          0.014403    -0.109733     0.002690   \n",
       "4   -0.090490     ...          0.049627    -0.031102     0.043574   \n",
       "\n",
       "   sent_vec_93  sent_vec_94  sent_vec_95  sent_vec_96  sent_vec_97  \\\n",
       "0    -0.015696    -0.039146    -0.034641    -0.026215    -0.079093   \n",
       "1     0.019743    -0.083541     0.018332    -0.020753    -0.027814   \n",
       "2     0.050657    -0.152419    -0.024303    -0.033989    -0.062024   \n",
       "3     0.046515    -0.238110     0.006557    -0.143425     0.060398   \n",
       "4    -0.036604    -0.119386    -0.016236    -0.116524    -0.016949   \n",
       "\n",
       "   sent_vec_98  sent_vec_99  \n",
       "0     0.068413     0.062398  \n",
       "1     0.011478     0.054295  \n",
       "2     0.138996     0.046701  \n",
       "3     0.095089     0.036426  \n",
       "4     0.043204     0.084453  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['sent_vec_'+str(i) for i in range(300)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
